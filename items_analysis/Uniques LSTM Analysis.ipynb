{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T14:52:48.376433Z",
     "start_time": "2019-03-05T14:52:48.373445Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # ignore messy numpy warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T14:52:48.685257Z",
     "start_time": "2019-03-05T14:52:48.682751Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T14:52:51.771817Z",
     "start_time": "2019-03-05T14:52:48.974615Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import pprint\n",
    "from scipy import stats\n",
    "import random\n",
    "from datetime import datetime\n",
    "import json\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from pandas import read_csv\n",
    "from datetime import datetime\n",
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import os.path\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T15:06:01.666818Z",
     "start_time": "2019-03-05T15:06:01.603738Z"
    }
   },
   "outputs": [],
   "source": [
    "def pre_process_df(df):\n",
    "    \n",
    "    #Drop unwanted columns\n",
    "    df_dropped = df.drop(columns=['Unnamed: 0',\n",
    "                                  'stash_id','stash_feed','item_category',\n",
    "                                  '_id',\n",
    "                                  'date_month','date_year','item_name',\n",
    "                                  'league','rarity',\n",
    "                                  'price_currency','price_raw','date_day',\n",
    "                                  'time_minutes'],errors='ignore')\n",
    "    \n",
    "    # delete columns with no values in them, which means only zero\n",
    "    proc_df = df_dropped.loc[:,(df!=0).any (axis=0)]\n",
    "        \n",
    "    # Clean up of data. \n",
    "    ## Prices were imported as 'Object' not 'float'. We need to convert them to float.\n",
    "    proc_df[['price_amount']] = pd.to_numeric(proc_df['price_amount'],errors='coerce')\n",
    "    \n",
    "    # Remove rows where price_amount is NaN\n",
    "    proc_df = proc_df[pd.notnull(proc_df['price_amount'])]\n",
    "    proc_df = proc_df.loc[proc_df['price_amount']!=0]\n",
    "    \n",
    "    return proc_df\n",
    "\n",
    "def split_df_to_unique_item_names(df):\n",
    "\n",
    "    unique_item_names = df['item_name'].value_counts(ascending=False)\n",
    "\n",
    "    ascending_dataframes_per_name = {}\n",
    "\n",
    "    for item_name in unique_item_names.index:\n",
    "        dataF = df.loc[(df['item_name']== item_name)]\n",
    "        if dataF.empty: continue\n",
    "        if item_name not in ascending_dataframes_per_name:    \n",
    "            ascending_dataframes_per_name[item_name] = dataF\n",
    "\n",
    "    for item_name in ascending_dataframes_per_name:\n",
    "        dataF = ascending_dataframes_per_name[item_name]\n",
    "        dataF = dataF.loc[:,(dataF!=0).any(axis=0)]\n",
    "        mask = dataF['price_amount'].notna()\n",
    "        dataF = dataF[mask]\n",
    "        ascending_dataframes_per_name[item_name] = dataF.reset_index()\n",
    "        \n",
    "    return ascending_dataframes_per_name\n",
    "\n",
    "def compute_corr(df,method='spearman',filename=''):\n",
    "    \n",
    "    min_periods = int(len(df))*0.1\n",
    "    cols = list(df.filter(regex='(Attacks per Second|Energy Shield|Elemental Damage|Critical Strike Chance|Physical Damage|influence|Armour|sockets_number|linked_sockets|Evasion Rating)|(?=^co_|ex_|im_|en_$)(^.*$)').columns.values)\n",
    "    df[cols] = df[cols].replace({0:np.nan, 0.0:np.nan})\n",
    "    #df[df.filter(regex='(?=^co_|ex_|im_|en_$)(^.*$)') <= 0.0] = np.nan\n",
    "    corr = df.corr(method,min_periods = min_periods)\n",
    "    corr = corr.dropna('columns',how='all')\n",
    "    corr = corr.dropna('rows',how='all')\n",
    "    \n",
    "    return corr\n",
    "\n",
    "def remove_outliers_IQR(item_dataframe,column_label = 'price_amount',high_quantile=0.75):\n",
    "    '''Function removes outliers from a dataframe along the price_amount column by default.\n",
    "\n",
    "    Input:\n",
    "        df: pandas DataFrame\n",
    "        column_label: along which column to check for outliers(default = 'price_amount')\n",
    "        high_quantile: high-end quantile to use on boxplot'''\n",
    "    \n",
    "    Q1 = item_dataframe[column_label].quantile(1-high_quantile)\n",
    "    Q3 = item_dataframe[column_label].quantile(high_quantile)\n",
    "    IQR = Q3 - Q1\n",
    "    new_df = item_dataframe[~((item_dataframe[column_label] < (Q1 - 1.5 * IQR))|(item_dataframe[column_label] > (Q3 + 1.5 * IQR)))]\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "def remove_outliers_zscore(item_dataframe,column_labels = ['price_amount'],threshold=3,show_results=False):\n",
    "    '''Function removes outliers using z-score from a dataframe along the price_amount column by default.\n",
    "\n",
    "    Input:\n",
    "        df: pandas DataFrame\n",
    "        column_label: along which columns to check for outliers(default = ['price_amount'])\n",
    "        show_results: show results before and after removing outliers(default = False)\n",
    "        size: vertical and horizontal size of the plot'''\n",
    "    \n",
    "    z_score = np.abs(stats.zscore(item_dataframe[column_labels]))\n",
    "    new_df = item_dataframe[(z_score < threshold)]\n",
    "    if show_results:\n",
    "        data_outliers_index = np.where(z_score > threshold)[0]\n",
    "        print('Data outliers for \"{}\":'.format(item_dataframe['item_name'][0]))\n",
    "        for id in data_outliers_index:\n",
    "              print('index: {:<10d}{}: {:<10f}'.format(id,column_labels[0],item_dataframe.iloc[id][column_labels[0]]))\n",
    "        print('Removed {} rows'.format(item_dataframe.shape[0]-new_df.shape[0]))\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def produce_decision_dataframe(item_df,correlations_df=pd.DataFrame,incl_outliers=True,method='z-score',threshold=2,quantile=0.8):\n",
    "    \n",
    "    d_df = item_df\n",
    "    \n",
    "    if not incl_outliers:\n",
    "        if method == 'z-score' : \n",
    "            d_df = remove_outliers_zscore(d_df,threshold=threshold)\n",
    "        elif method == 'IQR' :\n",
    "            d_df = remove_outliers_IQR(d_df,high_quantile=quantile)\n",
    "        else:\n",
    "            raise Exception('\\t\\tWrong outlier mode. Valid options mode = [z-score | IQR]')\n",
    "    \n",
    "    if correlations_df.empty:\n",
    "        columns = ['item_name','feature','corr_value','no_features','transactions','st_div','variance']\n",
    "        correlations_df =  pd.DataFrame(columns=columns)\n",
    "    \n",
    "    corr=compute_corr(d_df,method='kendall')\n",
    "    #corr_filtered = corr['price_amount'].filter(regex='(item_category|corrupted|Attacks per Second|Energy Shield|Elemental Damage|Critical Strike Chance|Physical Damage|influence|Armour|sockets_number|linked_sockets|Quality|Evasion Rating)|(?=^co_|ex_|im_|en_$)(^.*$)').drop(labels=['ex_conv_rate'],axis=0).dropna()\n",
    "    corr_filtered = corr['price_amount'].filter(regex='(date_day|item_category|corrupted|Attacks per Second|Energy Shield|Elemental Damage|Critical Strike Chance|Physical Damage|influence|Armour|sockets_number|linked_sockets|Quality|Evasion Rating)|(?=^co_|ex_|im_|en_$)(^.*$)').dropna()\n",
    "    for row in corr_filtered.index:\n",
    "        correlations_df = correlations_df.append({'item_name':d_df['item_name'].unique()[0],\n",
    "                                'feature':row,\n",
    "                                'corr_value': corr_filtered[row],\n",
    "                                'no_features':len(corr_filtered),\n",
    "                                'transactions':d_df.groupby('item_name')['item_name'].count().values[0],\n",
    "                                'st_div':d_df['price_amount'].describe()['std'],\n",
    "                                'variance':d_df[['price_amount']].var(axis=0)},ignore_index=True)\n",
    "    \n",
    "    return correlations_df\n",
    "\n",
    "def produce_corr_based_df(df_per_item_name,method='z-score',threshold=2,quantile=0.8):\n",
    "    \n",
    "    columns = ['item_name','feature','corr_value','no_features','transactions','st_div','variance']\n",
    "\n",
    "    df =  pd.DataFrame(columns=columns)\n",
    "\n",
    "    count = 0\n",
    "    for dataF in df_per_item_name:\n",
    "        count= count+1\n",
    "        if count%200==0:\n",
    "            print(\"Processed {} item_names\".format(count))\n",
    "        item_df = df_per_item_name[dataF]\n",
    "        df = produce_decision_dataframe(item_df,df,incl_outliers=False,method=method,threshold=threshold,quantile=quantile)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def filter_decision_df(df, days=7, min_corr=0.1, min_no_features=2, min_std=5.0):\n",
    "    \n",
    "    min_trx = days*24\n",
    "    \n",
    "    df_filtered = df[(abs(df['corr_value'])>=min_corr) & \\\n",
    "                                          (df['transactions'] > min_trx) & \\\n",
    "                                          (df['st_div'] > min_std)]\n",
    "    \n",
    "    df_filtered['no_features'] = df_filtered.groupby('item_name')['item_name'].transform('count')\n",
    "    df_filtered = df_filtered[df_filtered['no_features'] >= min_no_features]\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "def convert_column_values_string_to_rankInt(df) -> pd.DataFrame:\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == type(object):\n",
    "            le = preprocessing.LabelEncoder()\n",
    "            df[column] = le.fit_transform(df[column])\n",
    "    return df\n",
    "\n",
    "def flatten_column(df_column,method='median',round_base=2):\n",
    "    if method=='median':\n",
    "        return df_column.median()\n",
    "    elif method=='mean':\n",
    "        return round(df_column.mean(),round_base)\n",
    "    \n",
    "def interpolate_df(df,config):\n",
    "    feature_series = []\n",
    "    index = []\n",
    "    item_features = df.columns\n",
    "    for feature in item_features:\n",
    "        if feature in config['features']:\n",
    "            if config['features'][feature]=='median':\n",
    "                inter_series_f = flatten_column(df[feature],method='median')\n",
    "            elif config['features'][feature]=='mean':\n",
    "                inter_series_f = flatten_column(df[feature],method='mean')\n",
    "            else:\n",
    "                inter_series_f = flatten_column(df[feature],method=config['default_flatten'])\n",
    "        else:\n",
    "            inter_series_f = flatten_column(df[feature],method=config['default_flatten'])\n",
    "        \n",
    "        index.append(feature)\n",
    "        feature_series.append(inter_series_f)\n",
    "    return pd.Series(feature_series,index)\n",
    "\n",
    "def fill_and_plot(df,method='default',order=3):\n",
    "    if method in ['spline','polynomial']:\n",
    "        df_inter = df.interpolate(method=method,order=order)\n",
    "    else:\n",
    "        df_inter = df.interpolate(method=method)\n",
    "    plt.figure(figsize=(20,10))\n",
    "    df_inter['price_amount'].plot()\n",
    "    plt.legend([method])\n",
    "    return df_inter\n",
    "\n",
    "\n",
    "def fill_time_periods(df,method = 'pchip',order=3):\n",
    "\n",
    "    if method in ['spline','polynomial']:\n",
    "        df = df.interpolate(method=method,order=order)\n",
    "    else:\n",
    "        df = df.interpolate(method=method)\n",
    "    return df\n",
    "\n",
    "def feature_selection(df, method=\"decision_tree\",verbose=0,importance_threshold=0.15,max_no_of_features = 5):\n",
    "    important_features = []\n",
    "    train = df.copy()\n",
    "    new_df = df.drop(['price_amount','time','date','socket_colors','time_hours'],axis=1,errors='ignore')\n",
    "    if method=='decision_tree':\n",
    "        model = RandomForestRegressor(random_state=12,max_depth=100)\n",
    "        new_df = pd.get_dummies(new_df)\n",
    "        model.fit(new_df,train.price_amount)\n",
    "        features = new_df.columns\n",
    "        importances = model.feature_importances_\n",
    "        indices = np.argsort(importances)[-max_no_of_features:]  # top 10 features\n",
    "        if verbose:\n",
    "            for i in indices:\n",
    "                print(\"Feature : {:40} --->importance [{}]\".format(features[i][:40],importances[i].round(3)))\n",
    "        for i in indices:\n",
    "            if importances[i] > importance_threshold:\n",
    "                important_features.append(features[i])\n",
    "        important_features.append('price_amount')\n",
    "\n",
    "        return important_features\n",
    "    elif method=='rfe':\n",
    "        lreg = DecisionTreeRegressor()\n",
    "        rfe = RFE(lreg, max_no_of_features-1)\n",
    "        cols = new_df.columns\n",
    "        rfe = rfe.fit(new_df, train.price_amount)\n",
    "        sorted_ranking = sorted(zip(map(lambda x:round(x,5),rfe.ranking_),cols))\n",
    "        \n",
    "        for i in range(0,len(sorted_ranking)):\n",
    "            if verbose==1: print(\"Feature : {:40} has weight [{}]\".format(sorted_ranking[i][1][:35],sorted_ranking[i][0]))\n",
    "#             if (sorted_ranking[i][0] <= max_no_of_features*0.3 and (len(important_features)<(max_no_of_features))):    \n",
    "            if (len(important_features)<(max_no_of_features-1)):\n",
    "                important_features.append(sorted_ranking[i][1])\n",
    "                \n",
    "        important_features.append('price_amount')\n",
    "        return important_features\n",
    "    \n",
    "    \n",
    "def plot_results(predicted_data, true_data,index,directory,file_name):\n",
    "    filename = os.path.join(directory,'Point to point forecast_{}.png'.format(file_name))\n",
    "    fig = plt.figure(facecolor='white',figsize=(30,20))\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.title('Point to point forecast_{}'.format(file_name),fontsize=25)\n",
    "    ax.plot(true_data, label='True Data')\n",
    "    plt.plot(index[-len(true_data):],predicted_data, label='Prediction', linestyle='-',linewidth=3)\n",
    "    indexes_len = len(index[-len(true_data):])\n",
    "    modulo = indexes_len//10\n",
    "    ticks = [i for i in range(0,indexes_len) if i%modulo==0]\n",
    "    plt.xticks(ticks=ticks,fontsize=18,rotation=45)\n",
    "    ax.tick_params(direction='out',pad=15)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.xlabel('time_period',fontsize=25)\n",
    "    ax = plt.gca()\n",
    "    ax.xaxis.set_label_coords(0.5,-0.08)\n",
    "#     plt.legend(loc='upper right', prop={'size': 25})\n",
    "    fig.legend(loc=7,prop={'size': 25})\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(right=0.88)\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_results_full_seq(predicted_data, true_data,index,directory,file_name):\n",
    "    rmse = []\n",
    "    reshaped_predicted = np.reshape(predicted_data, (-1, 1))\n",
    "    for i in range(len(predicted_data)):\n",
    "        rmse.append(sqrt(mean_squared_error(reshaped_predicted[i], true_data[i])))\n",
    "    \n",
    "    filename = os.path.join(directory,'Full sequence forecast_{}.png'.format(file_name))\n",
    "    fig = plt.figure(facecolor='white',figsize=(30,20))\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.title('Full Sequence forecast_{}'.format(file_name),fontsize=25)\n",
    "    ax.plot(rmse, label='RMSE')\n",
    "    ax.plot(true_data, label='True Data')\n",
    "    plt.plot(index[-len(true_data):],predicted_data, label='Prediction', linestyle='-',linewidth=3)\n",
    "    indexes_len = len(index[-len(true_data):])\n",
    "    modulo = indexes_len//10\n",
    "    ticks = [i for i in range(0,indexes_len) if i%modulo==0]\n",
    "    plt.xticks(ticks=ticks,fontsize=18,rotation=45)\n",
    "    ax.tick_params(direction='out',pad=15)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.xlabel('time_period',fontsize=25)\n",
    "    ax = plt.gca()\n",
    "    ax.xaxis.set_label_coords(0.5,-0.08)\n",
    "    fig.legend(loc=7,prop={'size': 25})\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(right=0.88)\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def plot_results_multiple(predicted_data, true_data, prediction_len,index,directory,file_name):\n",
    "    filename = os.path.join(directory,'Multistep forecasts_{}.png'.format(file_name))\n",
    "    fig = plt.figure(facecolor='white',figsize=(30,20))\n",
    "    ax = fig.add_subplot(111)    \n",
    "    plt.title('Multistep forecasts_{}'.format(file_name),fontsize=25)\n",
    "    ax.plot(index[-len(true_data):],true_data, label='True Data', linestyle='-',linewidth=3)\n",
    "    indexes_len = len(index[-len(true_data):])\n",
    "#     modulo = indexes_len//10\n",
    "#     ticks = [i for i in range(0,indexes_len) if i%modulo==0]\n",
    "#     plt.xticks(ticks=ticks,rotation=45,fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.xlabel('time_period',fontsize=25)    \n",
    "    ax = plt.gca()\n",
    "    ax.xaxis.set_label_coords(0.5,-0.08)\n",
    "    ax.tick_params(direction='out',pad=15)\n",
    "    # Pad the list of predictions to shift it in the graph to it's correct start\n",
    "    ticks=[]\n",
    "    for i, data in enumerate(predicted_data):\n",
    "        padding = [None for p in range(i * prediction_len)]\n",
    "        starting_forecast_date_index = len(padding)\n",
    "        ending_forecast_date_index =len(padding) + prediction_len\n",
    "        ticks.append(index[-len(true_data):][starting_forecast_date_index])\n",
    "        ticks.append(index[-len(true_data):][ending_forecast_date_index])\n",
    "        plt.plot((padding + data), label='Prediction')\n",
    "#     plt.legend(loc='upper right', prop={'size': 25})\n",
    "    plt.xticks(ticks=ticks,rotation=70,fontsize=18)\n",
    "    fig.legend(loc=7,prop={'size': 25})\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(right=0.88)\n",
    "    plt.savefig(filename)\n",
    "    plt.show()    \n",
    "\n",
    "def append_to_csv_file(filename,index,string_row,rmse,mode='a',case=1):\n",
    "    if not os.path.isfile(filename) or mode=='w': \n",
    "        f = open(filename,'w')\n",
    "        if case==1:\n",
    "            f.write(\"{}^{}^{}^{}^{}^{}^{}^{}^{}^{}^rmse\\n\".format(\"index\",\"sequence_length\",\"train_test_split\",\"epochs\",\"neurons0\",\"neurons1\",\n",
    "                                                              \"dropout_rate\",\"activation_function\",\"optimizer\",\"learning_rate\"))\n",
    "        elif case==2:\n",
    "            f.write(\"{}^{}^{}^{}^{}^{}^{}^{}^{}^{}^{}^rmse\\n\".format(\"index\",\"sequence_length\",\"train_test_split\",\"epochs\",\"neurons0\",\"neurons1\",\"neurons2\",\n",
    "                                                              \"dropout_rate\",\"activation_function\",\"optimizer\",\"learning_rate\"))\n",
    "        f.close()\n",
    "        return\n",
    "    f = open(filename,'a')\n",
    "    f.write(index)\n",
    "    f.write('^')\n",
    "    f.write(string_row+'^{}\\n'.format(rmse))\n",
    "    f.close()\n",
    "\n",
    "def change_dict_path_value(dotted_path, org,value,delim='.'):\n",
    "    paths, current = dotted_path.split(sep=delim), org\n",
    "    for p in paths[:-1]:\n",
    "        if is_number(p) : current = current[int(p)]\n",
    "        else : current = current[p]\n",
    "    current[paths[-1]] = value\n",
    "    return org\n",
    "\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_params_abbrev(configs,case=1):\n",
    "    if case==2:\n",
    "        string_row = \"{}^{}^{}^{}^{}^{}^{}^{}^{}^{}\".format(configs['data']['sequence_length'],\n",
    "                                                      configs['data']['train_test_split'],\n",
    "                                                      configs['training']['epochs'],\n",
    "                                                      configs['model']['layers'][0]['neurons'],\n",
    "                                                      configs['model']['layers'][1]['neurons'],\n",
    "                                                      configs['model']['layers'][2]['neurons'],\n",
    "                                                      configs['model']['layers'][3]['rate'],\n",
    "                                                      configs['model']['layers'][4]['activation'],\n",
    "                                                      configs['model']['optimizer'],\n",
    "                                                      configs['model']['learning_rate'])\n",
    "        index =  \"sl{}_spl{}_ep{}_n0{}_n1{}_n2{}_dr{}_ac{}_opt{}_lr{}\".format(configs['data']['sequence_length'],\n",
    "                                                      configs['data']['train_test_split'],\n",
    "                                                      configs['training']['epochs'],\n",
    "                                                      configs['model']['layers'][0]['neurons'],\n",
    "                                                      configs['model']['layers'][1]['neurons'],\n",
    "                                                      configs['model']['layers'][2]['neurons'],\n",
    "                                                      configs['model']['layers'][3]['rate'],\n",
    "                                                      configs['model']['layers'][4]['activation'],\n",
    "                                                      configs['model']['optimizer'],\n",
    "                                                      configs['model']['learning_rate'])\n",
    "    elif case==1:\n",
    "        string_row = \"{}^{}^{}^{}^{}^{}^{}^{}^{}\".format(configs['data']['sequence_length'],\n",
    "                                                      configs['data']['train_test_split'],\n",
    "                                                      configs['training']['epochs'],\n",
    "                                                      configs['model']['layers'][0]['neurons'],\n",
    "                                                      configs['model']['layers'][1]['neurons'],\n",
    "                                                      configs['model']['layers'][2]['rate'],\n",
    "                                                      configs['model']['layers'][3]['activation'],\n",
    "                                                      configs['model']['optimizer'],\n",
    "                                                      configs['model']['learning_rate'])\n",
    "        index =  \"sl{}_spl{}_ep{}_n0{}_n1{}_dr{}_ac{}_opt{}_lr{}\".format(configs['data']['sequence_length'],\n",
    "                                                      configs['data']['train_test_split'],\n",
    "                                                      configs['training']['epochs'],\n",
    "                                                      configs['model']['layers'][0]['neurons'],\n",
    "                                                      configs['model']['layers'][1]['neurons'],\n",
    "                                                      configs['model']['layers'][2]['rate'],\n",
    "                                                      configs['model']['layers'][3]['activation'],\n",
    "                                                      configs['model']['optimizer'],\n",
    "                                                      configs['model']['learning_rate'])\n",
    "    return index,string_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T15:04:14.920170Z",
     "start_time": "2019-03-05T15:04:14.899145Z"
    },
    "code_folding": [
     106
    ]
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "\n",
    "class DataLoader():\n",
    "    \"\"\"A class for loading and transforming data for the lstm model\"\"\"\n",
    "\n",
    "    def __init__(self, filename, split,config,inter_config):\n",
    "        dataframe = pd.read_csv(filename,delimiter='^').round(2)\n",
    "        dataframe = self.prepare_data(dataframe,config,inter_config)\n",
    "        dataframe = self.rearrange_yhat_to_first_column(dataframe)\n",
    "        i_split = int(len(dataframe) * split)\n",
    "        self.indexes = dataframe.index\n",
    "        self.data_train = dataframe.values[:i_split]\n",
    "        self.data_test  = dataframe.values[i_split:]\n",
    "        self.len_train  = len(self.data_train)\n",
    "        self.len_test   = len(self.data_test)\n",
    "        self.len_train_windows = None\n",
    "        \n",
    "    def prepare_data(self,df,config,inter_config):\n",
    "\n",
    "        item_name = df['item_name'].unique()[0]\n",
    "        item_inter_conf = inter_config[item_name]\n",
    "        no_of_features = config['data']['no_of_features']\n",
    "        corr_threshold = config['data']['corr_threshold']\n",
    "        inter_frequency = item_inter_conf['frequency']\n",
    "\n",
    "        #remove unwanted columns and rows\n",
    "        df = pre_process_df(df)\n",
    "\n",
    "        #Remove outliers\n",
    "        outlier_conf = config['data']['outliers']\n",
    "        if outlier_conf['method']=='IQR':\n",
    "            df = remove_outliers_IQR(df,high_quantile=outlier_conf['high_quantile'])\n",
    "        if outlier_conf['method']=='z_score':\n",
    "            df = remove_outliers_zscore(df,threshold=outlier_conf['threshold'])\n",
    "\n",
    "        #Feature selection prep\n",
    "        important_features = feature_selection(df,method='rfe',verbose=0,importance_threshold=corr_threshold,max_no_of_features=no_of_features)\n",
    "        \n",
    "        #Make a dateTime type column to interpolate with later\n",
    "        d = df['date'] + '-'+df['time']\n",
    "        df['date_time'] =  pd.to_datetime(d, format='%Y-%m-%d-%H-%M')\n",
    "        df = df.set_index(\"date_time\")\n",
    "\n",
    "        #Actual feature selection\n",
    "        df = df[important_features]\n",
    "        self.no_features = df.shape[1]\n",
    "\n",
    "        #Interpolation\n",
    "        df1 = df.groupby(pd.Grouper(freq=inter_frequency,closed='left')).apply(lambda x: interpolate_df(x,item_inter_conf))\n",
    "        df1 = df1.resample(inter_frequency).asfreq()\n",
    "        filled_df = fill_time_periods(df1,item_inter_conf['fill_method']).round(2)\n",
    "\n",
    "\n",
    "        return filled_df\n",
    "\n",
    "    def rearrange_yhat_to_first_column(self,df,yhat_name='price_amount'):\n",
    "\n",
    "        rearranged_columns = ['price_amount']\n",
    "        for c in df.columns:\n",
    "            if c=='price_amount' :continue\n",
    "            rearranged_columns.append(c)\n",
    "        return df[rearranged_columns]\n",
    "    \n",
    "    def get_test_data(self, seq_len, normalise):\n",
    "        '''\n",
    "        Create x, y test data windows\n",
    "        Warning: batch method, not generative, make sure you have enough memory to\n",
    "        load data, otherwise reduce size of the training split.\n",
    "        '''\n",
    "        data_windows = []\n",
    "        for i in range(self.len_test - seq_len):\n",
    "            data_windows.append(self.data_test[i:i+seq_len])\n",
    "\n",
    "        data_windows = np.array(data_windows).astype(float)\n",
    "        data_windows = self.normalise_windows(data_windows, single_window=False) if normalise else data_windows\n",
    "\n",
    "        x = data_windows[:, :-1]\n",
    "        y = data_windows[:, -1, [0]]\n",
    "        return x,y\n",
    "\n",
    "    def get_train_data(self, seq_len, normalise):\n",
    "        '''\n",
    "        Create x, y train data windows\n",
    "        Warning: batch method, not generative, make sure you have enough memory to\n",
    "        load data, otherwise use generate_training_window() method.\n",
    "        '''\n",
    "        data_x = []\n",
    "        data_y = []\n",
    "        for i in range(self.len_train - seq_len):\n",
    "            x, y = self._next_window(i, seq_len, normalise)\n",
    "            data_x.append(x)\n",
    "            data_y.append(y)\n",
    "#         data_y = self.scaler_y.fit_transform(data_y)\n",
    "        return np.array(data_x), np.array(data_y)\n",
    "\n",
    "    def generate_train_batch(self, seq_len, batch_size, normalise):\n",
    "        '''Yield a generator of training data from filename on given list of cols split for train/test'''\n",
    "        i = 0\n",
    "        while i < (self.len_train - seq_len):\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            for b in range(batch_size):\n",
    "                if i >= (self.len_train - seq_len):\n",
    "                    # stop-condition for a smaller final batch if data doesn't divide evenly\n",
    "                    yield np.array(x_batch), np.array(y_batch)\n",
    "                    i = 0\n",
    "                x, y = self._next_window(i, seq_len, normalise)\n",
    "                x_batch.append(x)\n",
    "                y_batch.append(y)\n",
    "                i += 1\n",
    "            yield np.array(x_batch), np.array(y_batch)\n",
    "\n",
    "    def _next_window(self, i, seq_len, normalise):\n",
    "        '''Generates the next data window from the given index location i'''\n",
    "        window = self.data_train[i:i+seq_len]\n",
    "        window = self.normalise_windows(window, single_window=True)[0] if normalise else window\n",
    "        x = window[:-1]\n",
    "        y = window[-1, [0]]\n",
    "        return x, y\n",
    "\n",
    "    def normalise_windows(self, window_data, single_window=False):\n",
    "        '''Normalise window with a base value of zero'''\n",
    "        normalised_data = []\n",
    "        window_data = [window_data] if single_window else window_data\n",
    "        for window in window_data:\n",
    "            normalised_window = []\n",
    "            for col_i in range(window.shape[1]):\n",
    "                normalised_col = [((float(p) / float(window[0, col_i])) - 1) for p in window[:, col_i]]\n",
    "                normalised_window.append(normalised_col)\n",
    "            normalised_window = np.array(normalised_window).T # reshape and transpose array back into original multidimensional format\n",
    "            normalised_data.append(normalised_window)\n",
    "        return np.array(normalised_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T15:04:15.067952Z",
     "start_time": "2019-03-05T15:04:15.049936Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from numpy import newaxis\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "class Model():\n",
    "    \"\"\"A class for an building and inferencing an lstm model\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = Sequential()\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        print('[Model] Loading model from file %s' % filepath)\n",
    "        self.model = load_model(filepath)\n",
    "\n",
    "    def build_model(self, configs):\n",
    "        optimizer = configs['model']['optimizer']\n",
    "        for layer in configs['model']['layers']:\n",
    "            neurons = layer['neurons'] if 'neurons' in layer else None\n",
    "            dropout_rate = layer['rate'] if 'rate' in layer else None\n",
    "            activation = layer['activation'] if 'activation' in layer else None\n",
    "            return_seq = layer['return_seq'] if 'return_seq' in layer else None\n",
    "            input_timesteps = configs['data']['sequence_length']-1\n",
    "#             input_dim = layer['input_dim'] if 'input_dim' in layer else None\n",
    "            input_dim = configs['data']['no_of_features'] if ('input_dim' in layer and configs['data']['feature_selection']) else None\n",
    "            \n",
    "            learning_rate = configs['model']['learning_rate']\n",
    "            \n",
    "            if layer['type'] == 'dense':\n",
    "                self.model.add(Dense(neurons, activation=activation))\n",
    "            if layer['type'] == 'lstm':\n",
    "                self.model.add(LSTM(neurons, input_shape=(input_timesteps, input_dim), return_sequences=return_seq))\n",
    "            if layer['type'] == 'dropout':\n",
    "                self.model.add(Dropout(dropout_rate))\n",
    "\n",
    "        if optimizer == 'SGD':\n",
    "            optimizer = SGD(lr=learning_rate)\n",
    "        \n",
    "        self.model.compile(loss=configs['model']['loss'], optimizer=optimizer,metrics=['mse', 'mae', 'mape'])\n",
    "\n",
    "        print('[Model] Model Compiled')\n",
    "\n",
    "    def train(self, x, y, epochs, batch_size, save_dir,filename):\n",
    "        print('[Model] Training Started')\n",
    "        print('[Model] %s epochs, %s batch size' % (epochs, batch_size))\n",
    "\n",
    "        save_fname = os.path.join(save_dir, filename+'.h5')\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=3),\n",
    "            ModelCheckpoint(filepath=save_fname, monitor='val_loss', save_best_only=True)\n",
    "        ]\n",
    "        self.model.fit(\n",
    "            x,\n",
    "            y,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=callbacks,\n",
    "            verbose=0\n",
    "        )\n",
    "        self.model.save(save_fname)\n",
    "\n",
    "        print('[Model] Training Completed. Model saved as %s' % save_fname)\n",
    "\n",
    "    def predict_point_by_point(self, data):\n",
    "        #Predict each timestep given the last sequence of true data, in effect only predicting 1 step ahead each time\n",
    "        print('[Model] Predicting Point-by-Point...')\n",
    "        predicted = self.model.predict(data)\n",
    "        predicted = np.reshape(predicted, (predicted.size,))\n",
    "        return predicted\n",
    "\n",
    "    def predict_sequences_multiple(self, data, window_size, prediction_len):\n",
    "        #Predict sequence of 50 steps before shifting prediction run forward by 50 steps\n",
    "        print('[Model] Predicting Sequences Multiple...')\n",
    "        prediction_seqs = []\n",
    "        for i in range(int(len(data)/prediction_len)):\n",
    "            curr_frame = data[i*prediction_len]\n",
    "            predicted = []\n",
    "            for j in range(prediction_len):\n",
    "                predicted.append(self.model.predict(curr_frame[newaxis,:,:])[0,0])\n",
    "                curr_frame = curr_frame[1:]\n",
    "                curr_frame = np.insert(curr_frame, [window_size-2], predicted[-1], axis=0)\n",
    "            prediction_seqs.append(predicted)\n",
    "        return prediction_seqs\n",
    "\n",
    "    def predict_sequence_full(self, data, window_size):\n",
    "        #Shift the window by 1 new prediction each time, re-run predictions on new window\n",
    "        print('[Model] Predicting Full Sequence...')\n",
    "        curr_frame = data[0]\n",
    "        predicted = []\n",
    "        for i in range(len(data)):\n",
    "            predicted.append(self.model.predict(curr_frame[newaxis,:,:])[0,0])\n",
    "            curr_frame = curr_frame[1:]\n",
    "            curr_frame = np.insert(curr_frame, [window_size-2], predicted[-1], axis=0)\n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T15:04:16.628037Z",
     "start_time": "2019-03-05T15:04:16.620029Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_item_analysis(configs,inter_conf,index):\n",
    "\n",
    "    data = DataLoader(configs['data']['filename'],\n",
    "        configs['data']['train_test_split'],\n",
    "        configs,\n",
    "        inter_conf\n",
    "    )\n",
    "\n",
    "    model = Model()\n",
    "    model.build_model(configs)\n",
    "\n",
    "    x, y = data.get_train_data(seq_len=configs['data']['sequence_length'], normalise=configs['data']['normalise'])\n",
    "    \n",
    "    model.train(\n",
    "        x,\n",
    "        y,\n",
    "        epochs = configs['training']['epochs'],\n",
    "        batch_size = configs['training']['batch_size'],\n",
    "        save_dir = configs['model']['save_dir'],\n",
    "        filename=index\n",
    "    )\n",
    "\n",
    "    x_test, y_test = data.get_test_data(\n",
    "        seq_len=configs['data']['sequence_length'],\n",
    "        normalise=configs['data']['normalise']\n",
    "    )\n",
    "\n",
    "\n",
    "    predictions = model.predict_sequences_multiple(x_test, configs['data']['sequence_length'], configs['data']['sequence_length'])\n",
    "    plot_results_multiple(predictions, y_test, configs['data']['sequence_length'],data.indexes.strftime(\"%d-%m_%Hh\"),directory=configs['model']['save_dir'],file_name=index)\n",
    "\n",
    "    predictions_fullseq = model.predict_sequence_full(x_test, configs['data']['sequence_length'])\n",
    "    plot_results_full_seq(predictions_fullseq, y_test,data.indexes.strftime(\"%d-%m_%Hh\"),directory=configs['model']['save_dir'],file_name=index)\n",
    "    \n",
    "    \n",
    "    predictions = model.predict_point_by_point(x_test)\n",
    "\n",
    "    rmse = sqrt(mean_squared_error(y_test, predictions))\n",
    "\n",
    "    print('Test RMSE: %.3f' % rmse)\n",
    "\n",
    "    plot_results(predictions, y_test,data.indexes.strftime(\"%d-%m_%Hh\"),directory=configs['model']['save_dir'],file_name=index)\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabula multi analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T15:06:09.855728Z",
     "start_time": "2019-03-05T15:06:09.851713Z"
    }
   },
   "outputs": [],
   "source": [
    "configs = json.load(open('multi_tabula_config.json', 'r'))\n",
    "if not os.path.exists(configs['model']['save_dir']): os.makedirs(configs['model']['save_dir'])\n",
    "filename = os.path.join(configs['model']['save_dir'],'lstm_tabula_results.csv')\n",
    "append_to_csv_file(filename,'','',0,mode='w',case=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T18:28:46.847107Z",
     "start_time": "2019-03-05T18:23:33.474810Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# disable interactive jupyter notebook printing\n",
    "%%capture\n",
    "\n",
    "inter_conf = json.load(open('interpolation_config.json', 'r'))\n",
    "\n",
    "grid = ParameterGrid({\"data.sequence_length\": [i for i in range(8,10)],\n",
    "                      \"data.train_test_split\": [0.3,0.4,0.5],\n",
    "                      \"training.epochs\": [500],\n",
    "                     \"model.optimizer\":['adam'],\n",
    "                     \"model.layers.0.neurons\":[500],\n",
    "                     \"model.layers.1.neurons\":[500],\n",
    "                     \"model.layers.3.rate\":[0],\n",
    "                     \"model.layers.4.activation\":['linear',]\n",
    "                     })\n",
    "\n",
    "for param_list in list(grid):\n",
    "        \n",
    "    for key,value in param_list.items():\n",
    "        configs = change_dict_path_value(key,configs,value)\n",
    "    index,string_row = get_params_abbrev(configs,case=2)\n",
    "    \n",
    "    results_df = pd.read_csv(filename,sep='^',index_col='index')\n",
    "    if index in results_df.index: \n",
    "        print(\"Index --{}-- exists\".format(index))\n",
    "        continue\n",
    "    try:\n",
    "        rmse = run_item_analysis(configs,inter_conf,index)\n",
    "        append_to_csv_file(filename,index,string_row,rmse)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        if K.backend() == 'tensorflow':\n",
    "            K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T17:22:29.941816Z",
     "start_time": "2019-02-25T17:22:29.900766Z"
    }
   },
   "outputs": [],
   "source": [
    "configs = json.load(open('multi_tabula_config.json', 'r'))\n",
    "filename = os.path.join(configs['model']['save_dir'],'lstm_tabula_results.csv')\n",
    "df = pd.read_csv(filename,sep='^',index_col='index')\n",
    "df.sort_values('rmse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Windripper multi analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T08:47:05.117411Z",
     "start_time": "2019-02-26T08:47:05.112404Z"
    }
   },
   "outputs": [],
   "source": [
    "configs = json.load(open('multi_windripper_config.json', 'r'))\n",
    "if not os.path.exists(configs['model']['save_dir']): os.makedirs(configs['model']['save_dir'])\n",
    "filename = os.path.join(configs['model']['save_dir'],'lstm_windripper_results.csv')\n",
    "append_to_csv_file(filename,'','',0,mode='w',case=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T09:41:53.390772Z",
     "start_time": "2019-02-26T08:47:05.434256Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "inter_conf = json.load(open('interpolation_config.json', 'r'))\n",
    "\n",
    "grid = ParameterGrid({\"data.sequence_length\": [i for i in range(6,15)],\n",
    "                      \"data.train_test_split\": [0.3,0.4,0.5],\n",
    "                      \"training.epochs\": [100,200,300],\n",
    "                     \"model.optimizer\":['adam'],\n",
    "                     \"model.layers.0.neurons\":[100],\n",
    "                     \"model.layers.1.neurons\":[50],\n",
    "                     \"model.layers.2.rate\":np.arange(0.1,0.31,0.1),\n",
    "                     \"model.layers.3.activation\":['linear']\n",
    "                     })\n",
    "\n",
    "for param_list in list(grid):\n",
    "        \n",
    "    for key,value in param_list.items():\n",
    "        configs = change_dict_path_value(key,configs,value)\n",
    "    index,string_row = get_params_abbrev(configs,case=1)\n",
    "    \n",
    "    results_df = pd.read_csv(filename,sep='^',index_col='index')\n",
    "    if index in results_df.index: \n",
    "        print(\"Index --{}-- exists\".format(index))\n",
    "        continue\n",
    "    try:\n",
    "        rmse = run_item_analysis(configs,inter_conf,index)\n",
    "        append_to_csv_file(filename,index,string_row,rmse)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        if K.backend() == 'tensorflow':\n",
    "            K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = json.load(open('multi_windripper_config.json', 'r'))\n",
    "filename = os.path.join(configs['model']['save_dir'],'lstm_windripper_results.csv')\n",
    "df = pd.read_csv(filename,sep='^',index_col='index')\n",
    "df.sort_values('rmse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = json.load(open('multi_windripper_config.json', 'r'))\n",
    "if not os.path.exists(configs['model']['save_dir']): os.makedirs(configs['model']['save_dir'])\n",
    "\n",
    "# config = json.load(open('tabula_config.json', 'r'))\n",
    "inter_conf = json.load(open('interpolation_config.json', 'r'))\n",
    "\n",
    "grid = ParameterGrid({\"data.sequence_length\": [i for i in range(6,21)],\n",
    "                      \"data.no_of_features\":[2,3],\n",
    "                      \"data.train_test_split\": [i for i in range(0.3,0.8,0.05)],\n",
    "                      \"training.epochs\": [100, 200,300],\n",
    "                     \"data.outliers.high_quantile\":[i for i in range(0.6,0.91,0.1)],\n",
    "                     \"model.loss\":['mse','mae','rmse','rmae'],\n",
    "                     \"model.optimizer\":['adam','sgd'],\n",
    "                     \"model.learning_rate\":[0.01,0.05,0.1,0.2],\n",
    "                     \"model.layers.0.neurons\":[50,100,200,300],\n",
    "                     \"model.layers.0.input_timesteps\":[i for i in range(6,20)],\n",
    "                     \"model.layers.0.input_dim\":[2,3],\n",
    "                     \"model.layers.1.neurons\":[25,50,100,150],\n",
    "                     \"model.layers.2.rate\":[i for i in range(0.1,0.51,0.05)],\n",
    "                     \"model.layers.3.activation\":['linear','sigmoid','relu']\n",
    "                     })\n",
    "\n",
    "data = DataLoader(configs['data']['filename'],\n",
    "    configs['data']['train_test_split'],\n",
    "    configs,\n",
    "    inter_conf\n",
    ")\n",
    "\n",
    "model = Model()\n",
    "model.build_model(configs)\n",
    "\n",
    "x, y = data.get_train_data(seq_len=configs['data']['sequence_length'], normalise=configs['data']['normalise'])\n",
    "\n",
    "#in-memory training\n",
    "model.train(\n",
    "    x,\n",
    "    y,\n",
    "    epochs = configs['training']['epochs'],\n",
    "    batch_size = configs['training']['batch_size'],\n",
    "    save_dir = configs['model']['save_dir']\n",
    ")\n",
    "\n",
    "x_test, y_test = data.get_test_data(\n",
    "    seq_len=configs['data']['sequence_length'],\n",
    "    normalise=configs['data']['normalise']\n",
    ")\n",
    "\n",
    "predictions = model.predict_sequences_multiple(x_test, configs['data']['sequence_length'], configs['data']['sequence_length'])\n",
    "plot_results_multiple(predictions, y_test, configs['data']['sequence_length'],data.indexes.strftime(\"%d-%m_%Hh\"))\n",
    "\n",
    "# predictions = model.predict_sequence_full(x_test, configs['data']['sequence_length'])\n",
    "# plot_results(predictions, y_test)\n",
    "\n",
    "predictions = model.predict_point_by_point(x_test)\n",
    "\n",
    "rmse = sqrt(mean_squared_error(y_test, predictions))\n",
    "\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "\n",
    "plot_results(predictions, y_test,data.indexes.strftime(\"%d-%m_%Hh\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "230px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
