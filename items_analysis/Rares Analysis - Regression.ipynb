{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T17:19:27.156326Z",
     "start_time": "2019-03-09T17:19:27.148318Z"
    }
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cm-s-ipython .CodeMirror-matchingbracket { color: LimeGreen !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T08:58:40.945170Z",
     "start_time": "2019-03-11T08:58:40.933655Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import pprint\n",
    "from scipy import stats\n",
    "import random\n",
    "from datetime import datetime\n",
    "import json\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from pprint import pprint\n",
    "import time\n",
    "from pandas import read_csv\n",
    "from datetime import datetime\n",
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.utils import np_utils\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from keras.wrappers.scikit_learn import KerasRegressor,KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import keras\n",
    "from keras.optimizers import SGD\n",
    "from sklearn import metrics\n",
    "# Standard plotly imports\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "# Using plotly + cufflinks in offline mode\n",
    "import cufflinks\n",
    "cufflinks.go_offline(connected=True)\n",
    "init_notebook_mode(connected=True)\n",
    "import json\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "# from xgboost.sklearn import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T17:19:31.298976Z",
     "start_time": "2019-03-09T17:19:31.295972Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T17:19:31.304981Z",
     "start_time": "2019-03-09T17:19:31.300977Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T17:19:31.309989Z",
     "start_time": "2019-03-09T17:19:31.306986Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T17:20:12.061580Z",
     "start_time": "2019-03-09T17:20:12.058562Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras import backend as K "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T07:54:04.541481Z",
     "start_time": "2019-03-11T07:54:04.456370Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def pre_process_df(df,verbose = 0):\n",
    "    \n",
    "    if verbose==1: print(\"Columns before : {}\".format(df.shape[1]))\n",
    "    \n",
    "    df['time_period'] = df['time_hours']//8 + 1\n",
    "    \n",
    "    df = build_pseudo_mods(df)\n",
    "    \n",
    "    #Drop unwanted columns\n",
    "    df_dropped = df.drop(columns=['Unnamed: 0','stash_id','stash_feed','item_category','_id','date','time_hours',\n",
    "                                  'date_month','date_year','item_name','league','rarity','time','socket_colors',\n",
    "                                  'price_currency','price_raw','date_day','time_minutes'],errors='ignore')\n",
    "    \n",
    "    if verbose==1: print(\"Columns after base drop : {}\".format(df_dropped.shape[1]))\n",
    "        \n",
    "    # delete columns with no values in them, which means only zero\n",
    "    proc_df = df_dropped.loc[:,(df!=0).any (axis=0)]\n",
    "        \n",
    "    if verbose==1: pprint(\"Removed columns with no values in them : {}\".format(df_dropped.shape[1]-proc_df.shape[1]))\n",
    "    \n",
    "    # Clean up of data. \n",
    "    ## Prices were imported as 'Object' not 'float'. We need to convert them to float.\n",
    "    proc_df[['price_amount']] = pd.to_numeric(proc_df['price_amount'],errors='coerce')     \n",
    "    \n",
    "    # Remove rows where price_amount is NaN\n",
    "    proc_df = proc_df[pd.notnull(proc_df['price_amount'])]\n",
    "    proc_df = proc_df.loc[proc_df['price_amount']>0]\n",
    "    \n",
    "    return proc_df\n",
    "\n",
    "def number_of_resistances(x):\n",
    "    number_of_resistances = 0\n",
    "    single_resistance_mods = ['ex_#% to chaos resistance','ex_#% to cold resistance','ex_#% to fire resistance','ex_#% to lightning resistance']\n",
    "    triple_elemental_resistance_mods = ['co_#% to all elemental resistances','im_#% to all elemental resistances']\n",
    "    elemental_resistance_mods = ['co_#% to all elemental resistances','ex_#% to cold resistance','ex_#% to fire resistance','ex_#% to lightning resistance','im_#% to all elemental resistances']\n",
    "    \n",
    "    for mod in triple_elemental_resistance_mods:\n",
    "        if mod in x and x[mod]>0 : number_of_resistances = 3\n",
    "    for mod in single_resistance_mods:\n",
    "        if mod in x and x[mod]>0 : number_of_resistances = number_of_resistances+1\n",
    "    return number_of_resistances\n",
    "\n",
    "def number_of_ele_resistances(x):\n",
    "    number_of_ele_resistances = 0\n",
    "    single_resistance_mods = ['ex_#% to chaos resistance','ex_#% to cold resistance','ex_#% to fire resistance','ex_#% to lightning resistance']\n",
    "    triple_elemental_resistance_mods = ['co_#% to all elemental resistances','im_#% to all elemental resistances']\n",
    "    elemental_resistance_mods = ['co_#% to all elemental resistances','ex_#% to cold resistance','ex_#% to fire resistance','ex_#% to lightning resistance','im_#% to all elemental resistances']\n",
    "    single_ele_resistance_mods = ['ex_#% to cold resistance','ex_#% to fire resistance','ex_#% to lightning resistance']\n",
    "    \n",
    "    for mod in triple_elemental_resistance_mods:\n",
    "        if mod in x and x[mod]>0 : return 3\n",
    "    for mod in single_ele_resistance_mods:\n",
    "        if mod in x and x[mod]>0 : number_of_ele_resistances = number_of_ele_resistances+1\n",
    "    return number_of_ele_resistances\n",
    "\n",
    "def total_elemental_resistance(x):\n",
    "    total_ele_res = 0\n",
    "    single_resistance_mods = ['ex_#% to chaos resistance','ex_#% to cold resistance','ex_#% to fire resistance','ex_#% to lightning resistance']\n",
    "    triple_elemental_resistance_mods = ['co_#% to all elemental resistances','im_#% to all elemental resistances']\n",
    "    elemental_resistance_mods = ['co_#% to all elemental resistances','ex_#% to cold resistance','ex_#% to fire resistance','ex_#% to lightning resistance','im_#% to all elemental resistances']\n",
    "    single_ele_resistance_mods = ['ex_#% to cold resistance','ex_#% to fire resistance','ex_#% to lightning resistance']\n",
    "    \n",
    "    for mod in single_ele_resistance_mods:\n",
    "        if mod in x and x[mod]>0 : total_ele_res = total_ele_res + x[mod]\n",
    "    for mod in triple_elemental_resistance_mods:\n",
    "        if mod in x and x[mod]>0 : total_ele_res = total_ele_res + (3 * x[mod])\n",
    "    return total_ele_res\n",
    "\n",
    "def total_resistance(x):\n",
    "    total_res = 0\n",
    "    single_resistance_mods = ['ex_#% to chaos resistance','ex_#% to cold resistance','ex_#% to fire resistance','ex_#% to lightning resistance']\n",
    "    triple_elemental_resistance_mods = ['co_#% to all elemental resistances','im_#% to all elemental resistances']\n",
    "    elemental_resistance_mods = ['co_#% to all elemental resistances','ex_#% to cold resistance','ex_#% to fire resistance','ex_#% to lightning resistance','im_#% to all elemental resistances']\n",
    "    single_ele_resistance_mods = ['ex_#% to cold resistance','ex_#% to fire resistance','ex_#% to lightning resistance']\n",
    "    \n",
    "    for mod in single_resistance_mods:\n",
    "        if mod in x and x[mod]>0 : total_res = total_res + x[mod]\n",
    "    for mod in triple_elemental_resistance_mods:\n",
    "        if mod in x and x[mod]>0 : total_res = total_res + (3 * x[mod])\n",
    "    return total_res\n",
    "\n",
    "def build_pseudo_mods(df):\n",
    "    df['#_of_resistances'] = df.apply(lambda x : number_of_resistances(x),axis=1)\n",
    "    df['#_of_ele_resistances'] = df.apply(lambda x : number_of_ele_resistances(x),axis=1)\n",
    "    df['total_resistance'] = df.apply(lambda x : total_resistance(x),axis=1)\n",
    "    df['total_elemental_resistance'] = df.apply(lambda x : total_elemental_resistance(x),axis=1)\n",
    "                    \n",
    "    return df\n",
    "                    \n",
    "def split_df_to_unique_item_names(df):\n",
    "\n",
    "    unique_item_names = df['item_name'].value_counts(ascending=False)\n",
    "\n",
    "    ascending_dataframes_per_name = {}\n",
    "\n",
    "    for item_name in unique_item_names.index:\n",
    "        dataF = df.loc[(df['item_name']== item_name)]\n",
    "        if dataF.empty: continue\n",
    "        if item_name not in ascending_dataframes_per_name:    \n",
    "            ascending_dataframes_per_name[item_name] = dataF\n",
    "\n",
    "    for item_name in ascending_dataframes_per_name:\n",
    "        dataF = ascending_dataframes_per_name[item_name]\n",
    "        dataF = dataF.loc[:,(dataF!=0).any(axis=0)]\n",
    "        mask = dataF['price_amount'].notna()\n",
    "        dataF = dataF[mask]\n",
    "        ascending_dataframes_per_name[item_name] = dataF.reset_index()\n",
    "        \n",
    "    return ascending_dataframes_per_name\n",
    "\n",
    "def compute_corr(df,method='spearman',filename=''):\n",
    "    \n",
    "    min_periods = int(len(df))*0.1\n",
    "    cols = list(df.filter(regex='(Attacks per Second|Energy Shield|Elemental Damage|Critical Strike Chance|Physical Damage|influence|Armour|sockets_number|linked_sockets|Evasion Rating)|(?=^co_|ex_|im_|en_$)(^.*$)').columns.values)\n",
    "    df[cols] = df[cols].replace({0:np.nan, 0.0:np.nan})\n",
    "    #df[df.filter(regex='(?=^co_|ex_|im_|en_$)(^.*$)') <= 0.0] = np.nan\n",
    "    corr = df.corr(method,min_periods = min_periods)\n",
    "    corr = corr.dropna('columns',how='all')\n",
    "    corr = corr.dropna('rows',how='all')\n",
    "    \n",
    "    return corr\n",
    "\n",
    "def remove_outliers_IQR(item_dataframe,column_label = 'price_amount',high_quantile=0.75):\n",
    "    '''Function removes outliers from a dataframe along the price_amount column by default.\n",
    "\n",
    "    Input:\n",
    "        df: pandas DataFrame\n",
    "        column_label: along which column to check for outliers(default = 'price_amount')\n",
    "        high_quantile: high-end quantile to use on boxplot'''\n",
    "    \n",
    "    Q1 = item_dataframe[column_label].quantile(1-high_quantile)\n",
    "    Q3 = item_dataframe[column_label].quantile(high_quantile)\n",
    "    IQR = Q3 - Q1\n",
    "    new_df = item_dataframe[~((item_dataframe[column_label] < (Q1 - 1.5 * IQR))|(item_dataframe[column_label] > (Q3 + 1.5 * IQR)))]\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "def remove_outliers_zscore(item_dataframe,column_labels = ['price_amount'],threshold=3,show_results=False):\n",
    "    '''Function removes outliers using z-score from a dataframe along the price_amount column by default.\n",
    "\n",
    "    Input:\n",
    "        df: pandas DataFrame\n",
    "        column_label: along which columns to check for outliers(default = ['price_amount'])\n",
    "        show_results: show results before and after removing outliers(default = False)\n",
    "        size: vertical and horizontal size of the plot'''\n",
    "    \n",
    "    z_score = np.abs(stats.zscore(item_dataframe[column_labels]))\n",
    "    new_df = item_dataframe[(z_score < threshold)]\n",
    "    if show_results:\n",
    "        data_outliers_index = np.where(z_score > threshold)[0]\n",
    "        print('Data outliers for \"{}\":'.format(item_dataframe['item_name'][0]))\n",
    "        for id in data_outliers_index:\n",
    "              print('index: {:<10d}{}: {:<10f}'.format(id,column_labels[0],item_dataframe.iloc[id][column_labels[0]]))\n",
    "        print('Removed {} rows'.format(item_dataframe.shape[0]-new_df.shape[0]))\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def produce_decision_dataframe(item_df,correlations_df=pd.DataFrame,incl_outliers=True,method='z-score',threshold=2,quantile=0.8):\n",
    "    \n",
    "    d_df = item_df\n",
    "    \n",
    "    if not incl_outliers:\n",
    "        if method == 'z-score' : \n",
    "            d_df = remove_outliers_zscore(d_df,threshold=threshold)\n",
    "        elif method == 'IQR' :\n",
    "            d_df = remove_outliers_IQR(d_df,high_quantile=quantile)\n",
    "        else:\n",
    "            raise Exception('\\t\\tWrong outlier mode. Valid options mode = [z-score | IQR]')\n",
    "    \n",
    "    if correlations_df.empty:\n",
    "        columns = ['item_name','feature','corr_value','no_features','transactions','st_div','variance']\n",
    "        correlations_df =  pd.DataFrame(columns=columns)\n",
    "    \n",
    "    corr=compute_corr(d_df,method='kendall')\n",
    "    #corr_filtered = corr['price_amount'].filter(regex='(item_category|corrupted|Attacks per Second|Energy Shield|Elemental Damage|Critical Strike Chance|Physical Damage|influence|Armour|sockets_number|linked_sockets|Quality|Evasion Rating)|(?=^co_|ex_|im_|en_$)(^.*$)').drop(labels=['ex_conv_rate'],axis=0).dropna()\n",
    "    corr_filtered = corr['price_amount'].filter(regex='(date_day|item_category|corrupted|Attacks per Second|Energy Shield|Elemental Damage|Critical Strike Chance|Physical Damage|influence|Armour|sockets_number|linked_sockets|Quality|Evasion Rating)|(?=^co_|ex_|im_|en_$)(^.*$)').dropna()\n",
    "    for row in corr_filtered.index:\n",
    "        correlations_df = correlations_df.append({'item_name':d_df['item_name'].unique()[0],\n",
    "                                'feature':row,\n",
    "                                'corr_value': corr_filtered[row],\n",
    "                                'no_features':len(corr_filtered),\n",
    "                                'transactions':d_df.groupby('item_name')['item_name'].count().values[0],\n",
    "                                'st_div':d_df['price_amount'].describe()['std'],\n",
    "                                'variance':d_df[['price_amount']].var(axis=0)},ignore_index=True)\n",
    "    \n",
    "    return correlations_df\n",
    "\n",
    "def produce_corr_based_df(df_per_item_name,method='z-score',threshold=2,quantile=0.8):\n",
    "    \n",
    "    columns = ['item_name','feature','corr_value','no_features','transactions','st_div','variance']\n",
    "\n",
    "    df =  pd.DataFrame(columns=columns)\n",
    "\n",
    "    count = 0\n",
    "    for dataF in df_per_item_name:\n",
    "        count= count+1\n",
    "        if count%200==0:\n",
    "            print(\"Processed {} item_names\".format(count))\n",
    "        item_df = df_per_item_name[dataF]\n",
    "        df = produce_decision_dataframe(item_df,df,incl_outliers=False,method=method,threshold=threshold,quantile=quantile)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def filter_decision_df(df, days=7, min_corr=0.1, min_no_features=2, min_std=5.0):\n",
    "    \n",
    "    min_trx = days*24\n",
    "    \n",
    "    df_filtered = df[(abs(df['corr_value'])>=min_corr) & \\\n",
    "                                          (df['transactions'] > min_trx) & \\\n",
    "                                          (df['st_div'] > min_std)]\n",
    "    \n",
    "    df_filtered['no_features'] = df_filtered.groupby('item_name')['item_name'].transform('count')\n",
    "    df_filtered = df_filtered[df_filtered['no_features'] >= min_no_features]\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "def convert_column_values_string_to_rankInt(df) -> pd.DataFrame:\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == type(object):\n",
    "            le = preprocessing.LabelEncoder()\n",
    "            df[column] = le.fit_transform(df[column])\n",
    "    return df\n",
    "\n",
    "def flatten_column(df_column,method='median',round_base=2):\n",
    "    if method=='median':\n",
    "        return df_column.median()\n",
    "    elif method=='mean':\n",
    "        return round(df_column.mean(),round_base)\n",
    "    \n",
    "def interpolate_df(df,config):\n",
    "    feature_series = []\n",
    "    index = []\n",
    "    item_features = df.columns\n",
    "    for feature in item_features:\n",
    "        if feature in config['features']:\n",
    "            if config['features'][feature]=='median':\n",
    "                inter_series_f = flatten_column(df[feature],method='median')\n",
    "            elif config['features'][feature]=='mean':\n",
    "                inter_series_f = flatten_column(df[feature],method='mean')\n",
    "            else:\n",
    "                inter_series_f = flatten_column(df[feature],method=config['default_flatten'])\n",
    "        else:\n",
    "            inter_series_f = flatten_column(df[feature],method=config['default_flatten'])\n",
    "        \n",
    "        index.append(feature)\n",
    "        feature_series.append(inter_series_f)\n",
    "    return pd.Series(feature_series,index)\n",
    "\n",
    "def fill_and_plot(df,method='default',order=3):\n",
    "    if method in ['spline','polynomial']:\n",
    "        df_inter = df.interpolate(method=method,order=order)\n",
    "    else:\n",
    "        df_inter = df.interpolate(method=method)\n",
    "    plt.figure(figsize=(20,10))\n",
    "    df_inter['price_amount'].plot()\n",
    "    plt.legend([method])\n",
    "    return df_inter\n",
    "\n",
    "\n",
    "def fill_time_periods(df,method = 'pchip',order=3):\n",
    "\n",
    "    if method in ['spline','polynomial']:\n",
    "        df = df.interpolate(method=method,order=order)\n",
    "    else:\n",
    "        df = df.interpolate(method=method)\n",
    "    return df\n",
    "\n",
    "def feature_selection(df, method=\"decision_tree\",verbose=0,importance_threshold=0.15,max_no_of_features = 5):\n",
    "    important_features = []\n",
    "    print(\"Max number of features = {}\".format(max_no_of_features))\n",
    "    train = df.copy()\n",
    "    new_df = df.drop(['price_amount','time','date','socket_colors','time_hours','item_category'],axis=1,errors='ignore')\n",
    "    \n",
    "    if method=='custom':\n",
    "        df = new_df.copy()\n",
    "        df_missing = df!=0\n",
    "        df_missing_perc = df_missing.sum()/df.shape[0]\n",
    "        for value in df_missing_perc[df_missing_perc>=0.001].index.values:\n",
    "            important_features.append(value)\n",
    "        important_features.append('price_amount')\n",
    "        return important_features\n",
    "    elif method=='decision_tree':\n",
    "        model = RandomForestRegressor(random_state=20,max_depth=300)\n",
    "        new_df = pd.get_dummies(new_df)\n",
    "        model.fit(new_df,train.price_amount)\n",
    "        features = new_df.columns\n",
    "        importances = model.feature_importances_\n",
    "        indices = np.argsort(importances)[-max_no_of_features:]  # top 10 features\n",
    "        if verbose:\n",
    "            for i in indices:\n",
    "                print(\"Feature : {:40} --->importance [{}]\".format(features[i][:40],importances[i].round(3)))\n",
    "                      \n",
    "        for i in indices:\n",
    "            if importances[i] > importance_threshold:\n",
    "                important_features.append(features[i])\n",
    "        important_features.append('price_amount')\n",
    "        return important_features\n",
    "    \n",
    "    elif method=='rfe':\n",
    "        lreg = DecisionTreeRegressor()\n",
    "        rfe = RFE(lreg, max_no_of_features-1)\n",
    "        cols = new_df.columns\n",
    "        rfe = rfe.fit(new_df, train.price_amount)\n",
    "        sorted_ranking = sorted(zip(map(lambda x:round(x,5),rfe.ranking_),cols))\n",
    "        \n",
    "        for i in range(0,len(sorted_ranking)):\n",
    "            if verbose==1: print(\"Feature : {:40} has weight [{}]\".format(sorted_ranking[i][1][:35],sorted_ranking[i][0]))\n",
    "#             if (sorted_ranking[i][0] <= max_no_of_features*0.3 and (len(important_features)<(max_no_of_features))):    \n",
    "            if (len(important_features)<(max_no_of_features-1)):\n",
    "                important_features.append(sorted_ranking[i][1])\n",
    "                \n",
    "        important_features.append('price_amount')\n",
    "        return important_features\n",
    "    \n",
    "def dynamic_bins(df_bins, no_rows, min_percent=3, step=0.5, max_cum_step=50):\n",
    "    count = 0\n",
    "    left = 0\n",
    "    percent = 0\n",
    "    step_value = 0\n",
    "    bins = [0]\n",
    "    labels = []\n",
    "    for row in df_bins.iterrows():  \n",
    "\n",
    "        count = count + float(row[1]['count'])\n",
    "        right = float(row[0].right)  \n",
    "        calc_percent = (count / no_rows) * 100\n",
    "\n",
    "        #print(count, calc_percent)\n",
    "        if (calc_percent >= min_percent or step_value >= max_cum_step - 0.5):\n",
    "            bins.append(right)\n",
    "            labels.append('({},{}]'.format(left,right))\n",
    "#             print(\"{}\\t {}\\t {}\\t {}\".format(left,right, count,round(calc_percent,2) ))        \n",
    "            left = float(row[0].right)        \n",
    "            count = 0\n",
    "            step_value = 0\n",
    "            continue\n",
    "\n",
    "        step_value = step_value + step \n",
    "        \n",
    "    bins.append((right+max_cum_step))\n",
    "    labels.append('({},{}]'.format(left,right+max_cum_step))\n",
    "#     print(\"{}\\t {}\\t {}\\t {}\".format(left,right, count,round(calc_percent,2) ))       \n",
    "    return bins,labels\n",
    "\n",
    "\n",
    "def categorise_column(x,categories_dict):\n",
    "    no_of_good_features = 0\n",
    "    no_of_bad_features = 0\n",
    "    \n",
    "    for col,value in x.items():\n",
    "        if col in categories_dict and value>0:\n",
    "            if value>=categories_dict[col]['good_feature_value'] : \n",
    "                no_of_good_features = no_of_good_features +1\n",
    "            if value<=categories_dict[col]['bad_feature_value'] : \n",
    "                no_of_bad_features = no_of_bad_features +1\n",
    "                \n",
    "    return no_of_good_features,no_of_bad_features\n",
    "\n",
    "def categorise_features_to_columns(x,categories_dict):\n",
    "    no_of_good_features = 0\n",
    "    no_of_bad_features = 0\n",
    "    \n",
    "    no_of_good_features,no_of_bad_features = categorise_column(x,categories_dict)\n",
    "    \n",
    "    return pd.Series([no_of_good_features,no_of_bad_features])\n",
    "\n",
    "\n",
    "def calculate_accuracy_and_acc_per_bucket(X_test, y_test,model):\n",
    "    dict = {'<20%':[0 for i in range(0,7)],'<30%':[0 for i in range(0,7)],\n",
    "        '<40%':[0 for i in range(0,7)],'<50%':[0 for i in range(0,7)],\n",
    "        '>50%':[0 for i in range(0,7)]}\n",
    "\n",
    "    yhat = model.predict(X_test)\n",
    "    yhat = np.exp(yhat)\n",
    "    y_test_2 = np.exp(y_test)\n",
    "    for yi,yi_hat in zip(y_test_2,yhat):\n",
    "        error = abs(yi-yi_hat)/yi*100\n",
    "        price_bucket=0\n",
    "        if yi<=2:\n",
    "            price_bucket=0\n",
    "        elif yi<=5:\n",
    "            price_bucket=1\n",
    "        elif yi<=10:\n",
    "            price_bucket=2\n",
    "        elif yi<=30:\n",
    "            price_bucket=3\n",
    "        elif yi<=50:\n",
    "            price_bucket=4\n",
    "        elif yi<=150:\n",
    "            price_bucket=5\n",
    "        else:\n",
    "            price_bucket=6\n",
    "        if error <=20:\n",
    "            dict['<20%'][price_bucket] =dict['<20%'][price_bucket] +1\n",
    "        elif error <=30: \n",
    "            dict['<30%'][price_bucket] =dict['<30%'][price_bucket] +1\n",
    "        elif error <=40: \n",
    "            dict['<40%'][price_bucket] =dict['<40%'][price_bucket] +1\n",
    "        elif error <=50: \n",
    "            dict['<50%'][price_bucket] =dict['<50%'][price_bucket] +1\n",
    "        else: \n",
    "            dict['>50%'][price_bucket] =dict['>50%'][price_bucket] +1\n",
    "\n",
    "    accurracies_per_pricebucket = pd.DataFrame(dict,index=['(0,2]','(2,5]','(5,10]','(10,30]','(30,50]','(50,150]','(150,nan]'])\n",
    "    \n",
    "    count = 0\n",
    "    for yi,yi_hat in zip(y_test_2,yhat):\n",
    "        error = (abs(yi-yi_hat)/yi)*100\n",
    "        if error<=50:\n",
    "            count = count+1\n",
    "    accuracy = count / len(yhat)*100\n",
    "    print(accuracy)\n",
    "    \n",
    "    accurracies_per_pricebucket['<50% error'] = accurracies_per_pricebucket.apply(lambda x : total_accuracy_for_bucket(x),axis=1)  \n",
    "    \n",
    "    return accurracies_per_pricebucket\n",
    "\n",
    "\n",
    "def change_dict_path_value(dotted_path, org,value,delim='.'):\n",
    "    paths, current = dotted_path.split(sep=delim), org\n",
    "    for p in paths[:-1]:\n",
    "        if is_number(p) : current = current[int(p)]\n",
    "        else : current = current[p]\n",
    "    current[paths[-1]] = value\n",
    "    return org\n",
    "\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "def swish(x):\n",
    "   beta = 1.5 #1, 1.5 or 2\n",
    "   return beta * x * keras.backend.sigmoid(x)\n",
    "\n",
    "def total_accuracy_for_bucket(x):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for index,value in x.items():\n",
    "        if index =='<20%' or index =='<30%' or index =='<40%' or index =='<50%' :\n",
    "            correct = correct + value\n",
    "        total = total + value\n",
    "    return correct/total * 100\n",
    "\n",
    "\n",
    "def rearrange_yhat_to_first_column(df,yhat_name='price_amount'):\n",
    "\n",
    "    rearranged_columns = ['price_amount']\n",
    "    for c in df.columns:\n",
    "        if c=='price_amount' :continue\n",
    "        rearranged_columns.append(c)\n",
    "    return df[rearranged_columns]\n",
    "\n",
    "def split_X_Y(df):\n",
    "    rearranged_df_values = rearrange_yhat_to_first_column(df).values\n",
    "    X_data = rearranged_df_values[:,1:] \n",
    "    Y_data = rearranged_df_values[:,0]\n",
    "    return X_data,Y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T17:19:31.391593Z",
     "start_time": "2019-03-09T17:19:31.377075Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasRegressor,KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "class Model():\n",
    "\n",
    "    def __init__(self,configs,no_of_features):\n",
    "        self.configs = configs\n",
    "        self.no_of_features = no_of_features\n",
    "        self.layer_count = 1\n",
    "#         self.model = Sequential()\n",
    "\n",
    "    def build_model(self):\n",
    "        configs = self.configs\n",
    "        no_of_features = self.no_of_features\n",
    "        optimizer = configs['model']['optimizer']\n",
    "        self.model = Sequential()\n",
    "        for layer in configs['model']['layers']:\n",
    "            neurons = layer['neurons'] if 'neurons' in layer else None\n",
    "            dropout_rate = layer['rate'] if 'rate' in layer else None\n",
    "            activation = layer['activation'] if 'activation' in layer else None\n",
    "            return_seq = layer['return_seq'] if 'return_seq' in layer else None\n",
    "            input_timesteps = layer['input_timesteps'] if 'input_timesteps' in layer else None\n",
    "            input_dim = layer['input_dim'] if 'input_dim' in layer else None\n",
    "\n",
    "            if layer['type'] == 'dense':\n",
    "                if input_dim:\n",
    "                    self.model.add(Dense(neurons,input_dim=no_of_features, activation=activation))\n",
    "                    print(\"Layer {} -> Dense input layer\".format(self.layer_count))\n",
    "                    self.layer_count= self.layer_count+1\n",
    "                else:\n",
    "                    if activation =='swish' : self.model.add(Dense(neurons, activation=swish))\n",
    "                    else : self.model.add(Dense(neurons, activation=activation))\n",
    "                    if neurons==1 and activation: print(\"Layer {} -> Dense output layer. Activation -{}-\".format(self.layer_count,activation))\n",
    "                    else: print(\"Layer {} -> Dense hidden layer\".format(self.layer_count))\n",
    "                    self.layer_count= self.layer_count+1\n",
    "            if layer['type'] == 'lstm':\n",
    "                self.model.add(LSTM(neurons, input_shape=(input_timesteps, input_dim), return_sequences=return_seq))\n",
    "            if layer['type'] == 'dropout':\n",
    "                self.model.add(Dropout(dropout_rate))\n",
    "                print(\"Layer {} -> Dropout hidden layer\".format(self.layer_count))\n",
    "                self.layer_count= self.layer_count+1\n",
    "        if optimizer == 'SGD':\n",
    "            print(\"optimizer = SGD\")\n",
    "            optimizer = SGD(lr=0.01)\n",
    "        else:\n",
    "            print(\"Optimizer = {}\".format(optimizer))\n",
    "            \n",
    "        print(\"Loss = {}\".format(configs['model']['loss']))\n",
    "        self.model.compile(loss=configs['model']['loss'], optimizer=optimizer,metrics=['mse', 'mae', 'mape', 'cosine'])\n",
    "        \n",
    "        print('[Model] Model Compiled')\n",
    "        print(self.model.summary())\n",
    "        \n",
    "    \n",
    "#     def kfold_fit_model(self,X,Y,configs,no_of_features,regression=True,):\n",
    "#         epochs = configs['training']['epochs']\n",
    "#         batch_size = configs['training']['batch_size']\n",
    "#         seed = 7\n",
    "#         np.random.seed(seed)\n",
    "#         if regression:\n",
    "#             estimator = KerasRegressor(build_fn=self.build_model,epochs=epochs,batch_size=batch_size,verbose=1)\n",
    "#         else:\n",
    "#             estimator = KerasClassifier(build_fn=self.build_model,epochs=epochs,batch_size=batch_size,verbose=1)\n",
    "        \n",
    "# #         estimators = []\n",
    "# #         estimators.append(('standardize',MinMaxScaler(feature_range = (0, 1)) ))\n",
    "# #         estimators.append(('mlp',estimator))\n",
    "# #         pipeline = Pipeline(estimators)\n",
    "        \n",
    "#         kfold = KFold(n_splits=10, random_state=seed)\n",
    "#         results = cross_val_score(estimator, X, Y, cv=kfold)\n",
    "        \n",
    "#         print(\"Kfold results : %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n",
    "        \n",
    "    def fit_model(self,X_train,y_train,configs,no_of_features):\n",
    "        epochs = configs['training']['epochs']\n",
    "        batch_size = configs['training']['batch_size']\n",
    "        early_stop = EarlyStopping(monitor='mean_squared_error',\n",
    "                              min_delta=0,\n",
    "                              patience=round(epochs*0.2),\n",
    "                              verbose=0, mode='auto')\n",
    "        history = self.model.fit(X_train,y_train,batch_size=batch_size,nb_epoch=epochs,callbacks=[early_stop])\n",
    "        plt.figure(figsize=(20,10))\n",
    "        plt.plot(history.history['mean_squared_error'])\n",
    "        plt.figure(figsize=(20,10))\n",
    "        plt.plot(history.history['mean_absolute_error'])\n",
    "        plt.figure(figsize=(20,10))\n",
    "        plt.plot(history.history['mean_absolute_percentage_error'])\n",
    "        plt.show()\n",
    "        return history\n",
    "    \n",
    "    def predict_yhat(self,test_X):\n",
    "        return self.model.predict(test_X)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T17:19:31.396625Z",
     "start_time": "2019-03-09T17:19:31.392621Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# from keras.wrappers.scikit_learn import KerasRegressor,KerasClassifier\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "# class Model():\n",
    "\n",
    "#     def __init__(self,):\n",
    "#         self.model = Sequential()\n",
    "\n",
    "#     def build_model(self, configs,no_of_features):\n",
    "\n",
    "#         for layer in configs['model']['layers']:\n",
    "#             neurons = layer['neurons'] if 'neurons' in layer else None\n",
    "#             dropout_rate = layer['rate'] if 'rate' in layer else None\n",
    "#             activation = layer['activation'] if 'activation' in layer else None\n",
    "#             return_seq = layer['return_seq'] if 'return_seq' in layer else None\n",
    "#             input_timesteps = layer['input_timesteps'] if 'input_timesteps' in layer else None\n",
    "#             input_dim = layer['input_dim'] if 'input_dim' in layer else None\n",
    "\n",
    "#             if layer['type'] == 'dense':\n",
    "#                 print(\"Input dim = {}\".format(input_dim))\n",
    "#                 if input_dim:\n",
    "#                     self.model.add(Dense(neurons,input_dim=no_of_features, activation=activation))\n",
    "#                 else:\n",
    "#                     self.model.add(Dense(neurons, activation=activation))\n",
    "#             if layer['type'] == 'lstm':\n",
    "#                 self.model.add(LSTM(neurons, input_shape=(input_timesteps, input_dim), return_sequences=return_seq))\n",
    "#             if layer['type'] == 'dropout':\n",
    "#                 self.model.add(Dropout(dropout_rate))\n",
    "\n",
    "#         #self.model.compile(loss=configs['model']['loss'], optimizer=configs['model']['optimizer'],metrics=['mse', 'mae', 'mape'])\n",
    "        \n",
    "#         print('[Model] Model Compiled')\n",
    "        \n",
    "#     def kfold_fit_model(self,X,Y,configs,no_of_features,regression=True,):\n",
    "#         epochs = configs['training']['epochs']\n",
    "#         batch_size = configs['training']['batch_size']\n",
    "#         seed = 7\n",
    "#         np.random.seed(seed)\n",
    "#         if regression:\n",
    "#             estimator = KerasRegressor(build_fn=self.build_model(configs,no_of_features),epochs=epochs,batch_size=batch_size,verbose=1)\n",
    "#         else:\n",
    "#             estimator = KerasClassifier(build_fn=self.build_model(configs,no_of_features),epochs=epochs,batch_size=batch_size,verbose=1)\n",
    "        \n",
    "#         estimators = []\n",
    "#         estimators.append(('standardize',MinMaxScaler(feature_range = (0, 1)) ))\n",
    "#         estimators.append(('mlp',estimator))\n",
    "#         pipeline = Pipeline(estimators)\n",
    "        \n",
    "#         kfold = KFold(n_splits=10, random_state=seed)\n",
    "#         results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
    "        \n",
    "#         print(\"Kfold results : %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n",
    "        \n",
    "#     def fit_model(self,X_train,y_train,configs,regression=True):\n",
    "#         epochs = configs['training']['epochs']\n",
    "#         batch_size = configs['training']['batch_size']\n",
    "#         early_stop = EarlyStopping(monitor='mean_squared_error',\n",
    "#                               min_delta=0,\n",
    "#                               patience=round(epochs*0.2),\n",
    "#                               verbose=0, mode='auto')\n",
    "#         history = self.model.fit(X_train,y_train,batch_size=batch_size,nb_epoch=epochs,callbacks=[early_stop])\n",
    "#         plt.figure(figsize=(20,10))\n",
    "#         plt.plot(history.history['mean_squared_error'])\n",
    "#         plt.figure(figsize=(20,10))\n",
    "#         plt.plot(history.history['mean_absolute_error'])\n",
    "#         plt.figure(figsize=(20,10))\n",
    "#         plt.plot(history.history['mean_absolute_percentage_error'])\n",
    "#         plt.show()\n",
    "    \n",
    "#     def predict_yhat(self,test_X):\n",
    "#         return self.model.predict(test_X)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T09:35:28.058884Z",
     "start_time": "2019-03-11T09:35:28.037859Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    \"\"\"A class for loading and transforming data for the lstm model\"\"\"\n",
    "\n",
    "    def __init__(self, filename,config):\n",
    "        self.dataF = pd.read_csv(filename,delimiter='^').round(2)\n",
    "        self.prepared_data = self.prepare_data(self.dataF,config)\n",
    "        \n",
    "    def prepare_data(self,df,config):\n",
    "        \n",
    "        max_no_of_features = config['data']['no_of_features']\n",
    "        corr_threshold = config['data']['corr_threshold']\n",
    "        fs_method = config['data']['fs_method']\n",
    "        \n",
    "        #remove unwanted columns and rows\n",
    "        df = pre_process_df(df,verbose=0)\n",
    "                \n",
    "        #Remove outliers\n",
    "        outlier_conf = config['data']['outliers']\n",
    "        if outlier_conf['method']=='IQR':\n",
    "            print('removing outliers via IQR method\\nshape before : {}'.format(df.shape))\n",
    "            df = remove_outliers_IQR(df,high_quantile=outlier_conf['high_quantile'])\n",
    "            print('shape after : {}'.format(df.shape))\n",
    "        elif outlier_conf['method']=='zscore':\n",
    "            df = remove_outliers_zscore(df,threshold=outlier_conf['threshold'])\n",
    "        elif outlier_conf['method']=='3_ex':\n",
    "            df = df[df['price_amount'] // df['ex_conv_rate']<= 3]\n",
    "        elif outlier_conf['method']=='brute':\n",
    "            df = df[df['price_amount'] // df['ex_conv_rate']<= 3]\n",
    "            df = df[df['price_amount'] >=0.5]\n",
    "        elif outlier_conf['method']=='cut_low':\n",
    "            df = df[df['price_amount']]\n",
    "        \n",
    "        feature_category_dict = {}\n",
    "        with open('bad_good_features_categegorization.json','r') as fjson:\n",
    "            feature_category_dict = json.load(fjson)\n",
    "        \n",
    "        #if a column has a high percentage of missing values make all values higher than 0(zero) equal to 1\n",
    "        if config['data']['high_percentage_missing_values']:\n",
    "            dataF = df.drop('price_amount',axis=1).copy()\n",
    "            cols = dataF.columns\n",
    "            count = 0\n",
    "            for col in cols:\n",
    "                column_zero_percentage = (dataF[col]!=0).sum()/(len(dataF[col]))*100\n",
    "                if column_zero_percentage<5:\n",
    "                    dataF[col] = dataF[col].apply(lambda x: 1 if x>0 else 0)\n",
    "                    count= count+1\n",
    "            dataF['price_amount'] = df['price_amount']\n",
    "            df = dataF.copy()\n",
    "        \n",
    "        if config['data']['feature_selection']:\n",
    "            \n",
    "            #Feature selection prep\n",
    "            important_features = feature_selection(df,method=fs_method, verbose=1,importance_threshold=corr_threshold,max_no_of_features=max_no_of_features)\n",
    "\n",
    "            #Actual feature selection\n",
    "            df = df[important_features]\n",
    "            print(\"Shape before {}\".format(df.shape))\n",
    "            \n",
    "            df['no_of_ex_features'] = (df.filter(regex='ex_.+')!=0).sum(axis=1)-1\n",
    "            df = df[df['no_of_ex_features']>0]\n",
    "            \n",
    "            print(\"Shape after {}\".format(df.shape))\n",
    "            print('Feature Selection method works')\n",
    "            \n",
    "        df[['no_of_good_features','no_of_bad_features']] = df.apply(lambda x : categorise_features_to_columns(x,feature_category_dict),axis=1)    \n",
    "        \n",
    "        self.no_features = df.shape[1]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def rearrange_yhat_to_first_column(self,df,yhat_name='price_amount'):\n",
    "    \n",
    "        rearranged_columns = ['price_amount']\n",
    "        for c in df.columns:\n",
    "            if c=='price_amount' :continue\n",
    "            rearranged_columns.append(c)\n",
    "        return df[rearranged_columns]\n",
    "    \n",
    "    def split_X_Y(self):\n",
    "        rearranged_df_values = self.rearrange_yhat_to_first_column(self.prepared_data).values\n",
    "        X_data = rearranged_df_values[:,1:] \n",
    "        Y_data = rearranged_df_values[:,0]\n",
    "        return X_data,Y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T10:43:47.175864Z",
     "start_time": "2019-03-11T09:38:40.273768Z"
    }
   },
   "outputs": [],
   "source": [
    "configs = json.load(open('swish.json', 'r'))\n",
    "\n",
    "full_data = pd.read_csv(configs['data']['filename'],sep='^')\n",
    "\n",
    "X,y = split_X_Y(full_data)\n",
    "\n",
    "#np.log all the prices\n",
    "y = np.log(y)\n",
    "\n",
    "no_of_features = X.shape[1]\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.1)\n",
    "\n",
    "sc = MinMaxScaler(feature_range=(0,1))\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# (30,50] ----> 50 epochs, 45 features, Swish without \toutliers\n",
    "# (50,150] ----> 50 epochs, 45 features, Swish without\toutliers\n",
    "# (150,nan] ----> 50 epochs, 45 features, Swish without\toutliers\n",
    "# (5,10] ----> 50 epochs, 45 features, Swish without\t\toutliers\n",
    "\n",
    "configs['training']['epochs'] = 50\n",
    "configs['model']['layers'][5]['activation'] = 'swish'\n",
    "\n",
    "swish_model1 = Model(configs,X.shape[1])\n",
    "swish_model1.build_model()\n",
    "\n",
    "history = swish_model1.fit_model(X_train,y_train,configs,X.shape[1])\n",
    "\n",
    "# (0,2] ----> 30 epochs, 45 features, Swish without outliers \n",
    "\n",
    "configs['training']['epochs'] = 30\n",
    "configs['model']['layers'][5]['activation'] = 'swish'\n",
    "\n",
    "swish_model2 = Model(configs,X.shape[1])\n",
    "swish_model2.build_model()\n",
    "\n",
    "history = swish_model2.fit_model(X_train,y_train,configs,X.shape[1])\n",
    "\n",
    "\n",
    "# (2,5] ----> 30 epochs, 45 features, Sigmoid without\t\toutliers\n",
    "\n",
    "configs['training']['epochs'] = 30\n",
    "configs['model']['layers'][5]['activation'] = 'sigmoid'\n",
    "\n",
    "sigmoid_model1 = Model(configs,X.shape[1])\n",
    "sigmoid_model1.build_model()\n",
    "\n",
    "history = sigmoid_model1.fit_model(X_train,y_train,configs,X.shape[1])\n",
    "\n",
    "# (10,30] ----> 100 epochs, 45 features, Linear without\toutliers\n",
    "\n",
    "\n",
    "configs['training']['epochs'] = 100\n",
    "configs['model']['layers'][5]['activation'] = 'linear'\n",
    "\n",
    "linear_model1 = Model(configs,X.shape[1])\n",
    "linear_model1.build_model()\n",
    "\n",
    "history = linear_model1.fit_model(X_train,y_train,configs,X.shape[1])\n",
    "\n",
    "# pred1=swish_model1.predict(X_test)\n",
    "# pred2=swish_model2.predict(X_test)\n",
    "# pred3=sigmoid_model1.predict(X_test)\n",
    "# pred4=linear_model1.predict(X_test)\n",
    "\n",
    "# finalpred=(pred1+pred2+pred3+pred4)/4\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T12:44:03.794386Z",
     "start_time": "2019-03-11T12:43:56.218898Z"
    }
   },
   "outputs": [],
   "source": [
    "pred1=swish_model1.model.predict(X_test)\n",
    "pred2=swish_model2.model.predict(X_test)\n",
    "pred3=sigmoid_model1.model.predict(X_test)\n",
    "pred4=linear_model1.model.predict(X_test)\n",
    "\n",
    "finalpred=pred1*0.08+pred2*0.08+pred3*0.0005+pred4*0.82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T11:16:48.987339Z",
     "start_time": "2019-03-11T11:16:48.861681Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "finalpred = []\n",
    "for p1,p2,p3,p4 in zip(pred1,pred2,pred3,pred4):\n",
    "    arr = [p1[0],p2[0],p3[0],p4[0]]\n",
    "    predictions.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T11:16:53.133834Z",
     "start_time": "2019-03-11T11:16:52.577488Z"
    }
   },
   "outputs": [],
   "source": [
    "for pred in predictions:\n",
    "    min_num = 10000\n",
    "    num = 0\n",
    "    for i in range(0,len(pred)):\n",
    "        for j in range(i+1,len(pred)):\n",
    "            if ((abs(pred[j] - pred[i]) < min_num)):\n",
    "                min_num = pred[j] - pred[i]\n",
    "                num = (pred[j]+pred[i])/2\n",
    "    finalpred.append(num)\n",
    "#     finalpred=(pred1+pred2+pred4+pred3)/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T12:27:21.631960Z",
     "start_time": "2019-03-11T12:27:21.628952Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = [pred1,pred2,pred3,pred4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the best weights for ensembling learning using log_loss_fun with minimize function from scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T12:41:26.514772Z",
     "start_time": "2019-03-11T12:41:26.482817Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "\n",
    "def log_loss_func(weights):\n",
    "    ''' scipy minimize will pass the weights as a numpy array '''\n",
    "    final_prediction = 0\n",
    "    for weight, prediction in zip(weights, predictions):\n",
    "            final_prediction += weight*prediction\n",
    "\n",
    "    return mean_squared_error(y_test, final_prediction)\n",
    "    \n",
    "#the algorithms need a starting value, right not we chose 0.5 for all weights\n",
    "#its better to choose many random starting points and run minimize a few times\n",
    "starting_values = [0.15,0.15,0,0.7]\n",
    "\n",
    "#adding constraints  and a different solver as suggested by user 16universe\n",
    "#https://kaggle2.blob.core.windows.net/forum-message-attachments/75655/2393/otto%20model%20weights.pdf?sv=2012-02-12&se=2015-05-03T21%3A22%3A17Z&sr=b&sp=r&sig=rkeA7EJC%2BiQ%2FJ%2BcMpcA4lYQLFh6ubNqs2XAkGtFsAv0%3D\n",
    "cons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n",
    "#our weights are bound between 0 and 1\n",
    "bounds = [(0,1)]*len(predictions)\n",
    "\n",
    "res = minimize(log_loss_func, starting_values, method='L-BFGS-B', bounds=bounds, constraints=cons)\n",
    "\n",
    "print('Ensamble Score: {best_score}'.format(best_score=res['fun']))\n",
    "print('Best Weights: {weights}'.format(weights=res['x']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rare Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T08:53:23.271757Z",
     "start_time": "2019-03-11T08:30:21.166915Z"
    }
   },
   "outputs": [],
   "source": [
    "configs = json.load(open('chest_config.json', 'r'))\n",
    "\n",
    "data = DataLoader(configs['data']['filename'],configs)\n",
    "\n",
    "full_data = data.prepared_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T09:36:11.055049Z",
     "start_time": "2019-03-11T09:35:50.927081Z"
    }
   },
   "outputs": [],
   "source": [
    "full_data.to_csv(\"C:/Users/Digi/Desktop/Ensemble_Learning/45feat_without_outl_regression.csv\",sep='^')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T03:58:01.497437Z",
     "start_time": "2019-03-10T02:02:53.118893Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "all_accuracies_permutations = {}\n",
    "\n",
    "configs = json.load(open('chest_config.json', 'r'))\n",
    "\n",
    "grid = ParameterGrid({\"training.epochs\": [20,30,40],\n",
    "                      \"model.layers.5.activation\":['sigmoid','swish','linear'],\n",
    "                      \"data.outliers.method\":['IQR']})\n",
    "for param_list in list(grid):\n",
    "        \n",
    "    for key,value in param_list.items():\n",
    "        configs = change_dict_path_value(key,configs,value)\n",
    "    \n",
    "    if K.backend() == 'tensorflow':\n",
    "        K.clear_session()\n",
    "\n",
    "    data = DataLoader(configs['data']['filename'],configs)\n",
    "\n",
    "    full_data = data.prepared_data.copy()    \n",
    "        \n",
    "    data.prepared_data = full_data[(full_data['days_in_snapshot']<=10) &(full_data['days_in_snapshot']>=0)]\n",
    "\n",
    "    X,y = data.split_X_Y()\n",
    "\n",
    "    #np.log all the prices\n",
    "    y = np.log(y)\n",
    "\n",
    "    no_of_features = X.shape[1]\n",
    "\n",
    "    \n",
    "    permutation_key = \"epochs{}_acfunc{}_features{}_outliers{}\".format(configs['training']['epochs'],\n",
    "                                                           configs['model']['layers'][5]['activation'],\n",
    "                                                           no_of_features,configs['data']['outliers']['method'])\n",
    "    \n",
    "    \n",
    "#     configs = json.load(open('chest_config.json', 'r'))\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.1)\n",
    "    # sc = StandardScaler()\n",
    "    sc = MinMaxScaler(feature_range=(0,1))\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "    print(\"X.shape[1] = {}\".format(X.shape[1]))\n",
    "    model = Model(configs,X.shape[1])\n",
    "    model.build_model()\n",
    "    history = model.fit_model(X_train,y_train,configs,X.shape[1]) \n",
    "    \n",
    "    accuracies_per_bucket = calculate_accuracy_and_acc_per_bucket(X_test,y_test,model.model)\n",
    "    all_accuracies_permutations[permutation_key] = accuracies_per_bucket\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T08:32:39.868189Z",
     "start_time": "2019-03-10T08:32:39.795919Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for key, value in all_accuracies_permutations.items():\n",
    "    key\n",
    "    value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "234px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
