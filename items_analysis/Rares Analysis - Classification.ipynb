{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T07:30:28.109176Z",
     "start_time": "2019-03-11T07:30:28.079010Z"
    }
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cm-s-ipython .CodeMirror-matchingbracket { color: LimeGreen !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T07:30:32.988548Z",
     "start_time": "2019-03-11T07:30:28.613910Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import pprint\n",
    "from scipy import stats\n",
    "import random\n",
    "from datetime import datetime\n",
    "import json\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from pprint import pprint\n",
    "import time\n",
    "from pandas import read_csv\n",
    "from datetime import datetime\n",
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.utils import np_utils\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from keras.wrappers.scikit_learn import KerasRegressor,KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.optimizers import SGD\n",
    "from sklearn import metrics\n",
    "import keras\n",
    "import json\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from xgboost.sklearn import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T07:30:32.992551Z",
     "start_time": "2019-03-11T07:30:32.990548Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T07:30:32.997559Z",
     "start_time": "2019-03-11T07:30:32.994055Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T07:30:33.002065Z",
     "start_time": "2019-03-11T07:30:32.999561Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T13:41:47.955895Z",
     "start_time": "2019-03-09T13:41:47.897826Z"
    }
   },
   "outputs": [],
   "source": [
    "def pre_process_df(df,verbose = 0):\n",
    "    \n",
    "    if verbose==1: print(\"Columns before : {}\".format(df.shape[1]))\n",
    "    \n",
    "    df['time_period'] = df['time_hours']//8 + 1\n",
    "    \n",
    "    df = build_pseudo_mods(df)\n",
    "    \n",
    "    #Drop unwanted columns\n",
    "    df_dropped = df.drop(columns=['Unnamed: 0','stash_id','stash_feed','item_category','_id','date','time_hours',\n",
    "                                  'date_month','date_year','item_name','league','rarity','time','socket_colors',\n",
    "                                  'price_currency','price_raw','date_day','time_minutes'],errors='ignore')\n",
    "    \n",
    "    if verbose==1: print(\"Columns after base drop : {}\".format(df_dropped.shape[1]))\n",
    "        \n",
    "    # delete columns with no values in them, which means only zero\n",
    "    proc_df = df_dropped.loc[:,(df!=0).any (axis=0)]\n",
    "        \n",
    "    if verbose==1: pprint(\"Removed columns with no values in them : {}\".format(df_dropped.shape[1]-proc_df.shape[1]))\n",
    "    \n",
    "    # Clean up of data. \n",
    "    ## Prices were imported as 'Object' not 'float'. We need to convert them to float.\n",
    "    proc_df[['price_amount']] = pd.to_numeric(proc_df['price_amount'],errors='coerce')     \n",
    "    \n",
    "    # Remove rows where price_amount is NaN\n",
    "    proc_df = proc_df[pd.notnull(proc_df['price_amount'])]\n",
    "    proc_df = proc_df.loc[proc_df['price_amount']>0]\n",
    "    \n",
    "    return proc_df\n",
    "\n",
    "def number_of_resistances(x):\n",
    "    number_of_resistances = 0\n",
    "    single_resistance_mods = ['ex_#% to chaos resistance','ex_#% to cold resistance','ex_#% to fire resistance','ex_#% to lightning resistance']\n",
    "    triple_elemental_resistance_mods = ['co_#% to all elemental resistances','im_#% to all elemental resistances']\n",
    "    elemental_resistance_mods = ['co_#% to all elemental resistances','ex_#% to cold resistance','ex_#% to fire resistance','ex_#% to lightning resistance','im_#% to all elemental resistances']\n",
    "    \n",
    "    for mod in triple_elemental_resistance_mods:\n",
    "        if mod in x and x[mod]>0 : number_of_resistances = 3\n",
    "    for mod in single_resistance_mods:\n",
    "        if mod in x and x[mod]>0 : number_of_resistances = number_of_resistances+1\n",
    "    return number_of_resistances\n",
    "\n",
    "def number_of_ele_resistances(x):\n",
    "    number_of_ele_resistances = 0\n",
    "    single_resistance_mods = ['ex_#% to chaos resistance','ex_#% to cold resistance','ex_#% to fire resistance','ex_#% to lightning resistance']\n",
    "    triple_elemental_resistance_mods = ['co_#% to all elemental resistances','im_#% to all elemental resistances']\n",
    "    elemental_resistance_mods = ['co_#% to all elemental resistances','ex_#% to cold resistance','ex_#% to fire resistance','ex_#% to lightning resistance','im_#% to all elemental resistances']\n",
    "    single_ele_resistance_mods = ['ex_#% to cold resistance','ex_#% to fire resistance','ex_#% to lightning resistance']\n",
    "    \n",
    "    for mod in triple_elemental_resistance_mods:\n",
    "        if mod in x and x[mod]>0 : return 3\n",
    "    for mod in single_ele_resistance_mods:\n",
    "        if mod in x and x[mod]>0 : number_of_ele_resistances = number_of_ele_resistances+1\n",
    "    return number_of_ele_resistances\n",
    "\n",
    "def total_elemental_resistance(x):\n",
    "    total_ele_res = 0\n",
    "    single_resistance_mods = ['ex_#% to chaos resistance','ex_#% to cold resistance','ex_#% to fire resistance','ex_#% to lightning resistance']\n",
    "    triple_elemental_resistance_mods = ['co_#% to all elemental resistances','im_#% to all elemental resistances']\n",
    "    elemental_resistance_mods = ['co_#% to all elemental resistances','ex_#% to cold resistance','ex_#% to fire resistance','ex_#% to lightning resistance','im_#% to all elemental resistances']\n",
    "    single_ele_resistance_mods = ['ex_#% to cold resistance','ex_#% to fire resistance','ex_#% to lightning resistance']\n",
    "    \n",
    "    for mod in single_ele_resistance_mods:\n",
    "        if mod in x and x[mod]>0 : total_ele_res = total_ele_res + x[mod]\n",
    "    for mod in triple_elemental_resistance_mods:\n",
    "        if mod in x and x[mod]>0 : total_ele_res = total_ele_res + (3 * x[mod])\n",
    "    return total_ele_res\n",
    "\n",
    "def total_resistance(x):\n",
    "    total_res = 0\n",
    "    single_resistance_mods = ['ex_#% to chaos resistance','ex_#% to cold resistance','ex_#% to fire resistance','ex_#% to lightning resistance']\n",
    "    triple_elemental_resistance_mods = ['co_#% to all elemental resistances','im_#% to all elemental resistances']\n",
    "    elemental_resistance_mods = ['co_#% to all elemental resistances','ex_#% to cold resistance','ex_#% to fire resistance','ex_#% to lightning resistance','im_#% to all elemental resistances']\n",
    "    single_ele_resistance_mods = ['ex_#% to cold resistance','ex_#% to fire resistance','ex_#% to lightning resistance']\n",
    "    \n",
    "    for mod in single_resistance_mods:\n",
    "        if mod in x and x[mod]>0 : total_res = total_res + x[mod]\n",
    "    for mod in triple_elemental_resistance_mods:\n",
    "        if mod in x and x[mod]>0 : total_res = total_res + (3 * x[mod])\n",
    "    return total_res\n",
    "\n",
    "def build_pseudo_mods(df):\n",
    "    df['#_of_resistances'] = df.apply(lambda x : number_of_resistances(x),axis=1)\n",
    "    df['#_of_ele_resistances'] = df.apply(lambda x : number_of_ele_resistances(x),axis=1)\n",
    "    df['total_resistance'] = df.apply(lambda x : total_resistance(x),axis=1)\n",
    "    df['total_elemental_resistance'] = df.apply(lambda x : total_elemental_resistance(x),axis=1)\n",
    "                    \n",
    "    return df\n",
    "                    \n",
    "def split_df_to_unique_item_names(df):\n",
    "\n",
    "    unique_item_names = df['item_name'].value_counts(ascending=False)\n",
    "\n",
    "    ascending_dataframes_per_name = {}\n",
    "\n",
    "    for item_name in unique_item_names.index:\n",
    "        dataF = df.loc[(df['item_name']== item_name)]\n",
    "        if dataF.empty: continue\n",
    "        if item_name not in ascending_dataframes_per_name:    \n",
    "            ascending_dataframes_per_name[item_name] = dataF\n",
    "\n",
    "    for item_name in ascending_dataframes_per_name:\n",
    "        dataF = ascending_dataframes_per_name[item_name]\n",
    "        dataF = dataF.loc[:,(dataF!=0).any(axis=0)]\n",
    "        mask = dataF['price_amount'].notna()\n",
    "        dataF = dataF[mask]\n",
    "        ascending_dataframes_per_name[item_name] = dataF.reset_index()\n",
    "        \n",
    "    return ascending_dataframes_per_name\n",
    "\n",
    "def compute_corr(df,method='spearman',filename=''):\n",
    "    \n",
    "    min_periods = int(len(df))*0.1\n",
    "    cols = list(df.filter(regex='(Attacks per Second|Energy Shield|Elemental Damage|Critical Strike Chance|Physical Damage|influence|Armour|sockets_number|linked_sockets|Evasion Rating)|(?=^co_|ex_|im_|en_$)(^.*$)').columns.values)\n",
    "    df[cols] = df[cols].replace({0:np.nan, 0.0:np.nan})\n",
    "    #df[df.filter(regex='(?=^co_|ex_|im_|en_$)(^.*$)') <= 0.0] = np.nan\n",
    "    corr = df.corr(method,min_periods = min_periods)\n",
    "    corr = corr.dropna('columns',how='all')\n",
    "    corr = corr.dropna('rows',how='all')\n",
    "    \n",
    "    return corr\n",
    "\n",
    "def remove_outliers_IQR(item_dataframe,column_label = 'price_amount',high_quantile=0.75):\n",
    "    '''Function removes outliers from a dataframe along the price_amount column by default.\n",
    "\n",
    "    Input:\n",
    "        df: pandas DataFrame\n",
    "        column_label: along which column to check for outliers(default = 'price_amount')\n",
    "        high_quantile: high-end quantile to use on boxplot'''\n",
    "    \n",
    "    Q1 = item_dataframe[column_label].quantile(1-high_quantile)\n",
    "    Q3 = item_dataframe[column_label].quantile(high_quantile)\n",
    "    IQR = Q3 - Q1\n",
    "    new_df = item_dataframe[~((item_dataframe[column_label] < (Q1 - 1.5 * IQR))|(item_dataframe[column_label] > (Q3 + 1.5 * IQR)))]\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "def remove_outliers_zscore(item_dataframe,column_labels = ['price_amount'],threshold=3,show_results=False):\n",
    "    '''Function removes outliers using z-score from a dataframe along the price_amount column by default.\n",
    "\n",
    "    Input:\n",
    "        df: pandas DataFrame\n",
    "        column_label: along which columns to check for outliers(default = ['price_amount'])\n",
    "        show_results: show results before and after removing outliers(default = False)\n",
    "        size: vertical and horizontal size of the plot'''\n",
    "    \n",
    "    z_score = np.abs(stats.zscore(item_dataframe[column_labels]))\n",
    "    new_df = item_dataframe[(z_score < threshold)]\n",
    "    if show_results:\n",
    "        data_outliers_index = np.where(z_score > threshold)[0]\n",
    "        print('Data outliers for \"{}\":'.format(item_dataframe['item_name'][0]))\n",
    "        for id in data_outliers_index:\n",
    "              print('index: {:<10d}{}: {:<10f}'.format(id,column_labels[0],item_dataframe.iloc[id][column_labels[0]]))\n",
    "        print('Removed {} rows'.format(item_dataframe.shape[0]-new_df.shape[0]))\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def produce_decision_dataframe(item_df,correlations_df=pd.DataFrame,incl_outliers=True,method='z-score',threshold=2,quantile=0.8):\n",
    "    \n",
    "    d_df = item_df\n",
    "    \n",
    "    if not incl_outliers:\n",
    "        if method == 'z-score' : \n",
    "            d_df = remove_outliers_zscore(d_df,threshold=threshold)\n",
    "        elif method == 'IQR' :\n",
    "            d_df = remove_outliers_IQR(d_df,high_quantile=quantile)\n",
    "        else:\n",
    "            raise Exception('\\t\\tWrong outlier mode. Valid options mode = [z-score | IQR]')\n",
    "    \n",
    "    if correlations_df.empty:\n",
    "        columns = ['item_name','feature','corr_value','no_features','transactions','st_div','variance']\n",
    "        correlations_df =  pd.DataFrame(columns=columns)\n",
    "    \n",
    "    corr=compute_corr(d_df,method='kendall')\n",
    "    #corr_filtered = corr['price_amount'].filter(regex='(item_category|corrupted|Attacks per Second|Energy Shield|Elemental Damage|Critical Strike Chance|Physical Damage|influence|Armour|sockets_number|linked_sockets|Quality|Evasion Rating)|(?=^co_|ex_|im_|en_$)(^.*$)').drop(labels=['ex_conv_rate'],axis=0).dropna()\n",
    "    corr_filtered = corr['price_amount'].filter(regex='(date_day|item_category|corrupted|Attacks per Second|Energy Shield|Elemental Damage|Critical Strike Chance|Physical Damage|influence|Armour|sockets_number|linked_sockets|Quality|Evasion Rating)|(?=^co_|ex_|im_|en_$)(^.*$)').dropna()\n",
    "    for row in corr_filtered.index:\n",
    "        correlations_df = correlations_df.append({'item_name':d_df['item_name'].unique()[0],\n",
    "                                'feature':row,\n",
    "                                'corr_value': corr_filtered[row],\n",
    "                                'no_features':len(corr_filtered),\n",
    "                                'transactions':d_df.groupby('item_name')['item_name'].count().values[0],\n",
    "                                'st_div':d_df['price_amount'].describe()['std'],\n",
    "                                'variance':d_df[['price_amount']].var(axis=0)},ignore_index=True)\n",
    "    \n",
    "    return correlations_df\n",
    "\n",
    "def produce_corr_based_df(df_per_item_name,method='z-score',threshold=2,quantile=0.8):\n",
    "    \n",
    "    columns = ['item_name','feature','corr_value','no_features','transactions','st_div','variance']\n",
    "\n",
    "    df =  pd.DataFrame(columns=columns)\n",
    "\n",
    "    count = 0\n",
    "    for dataF in df_per_item_name:\n",
    "        count= count+1\n",
    "        if count%200==0:\n",
    "            print(\"Processed {} item_names\".format(count))\n",
    "        item_df = df_per_item_name[dataF]\n",
    "        df = produce_decision_dataframe(item_df,df,incl_outliers=False,method=method,threshold=threshold,quantile=quantile)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def filter_decision_df(df, days=7, min_corr=0.1, min_no_features=2, min_std=5.0):\n",
    "    \n",
    "    min_trx = days*24\n",
    "    \n",
    "    df_filtered = df[(abs(df['corr_value'])>=min_corr) & \\\n",
    "                                          (df['transactions'] > min_trx) & \\\n",
    "                                          (df['st_div'] > min_std)]\n",
    "    \n",
    "    df_filtered['no_features'] = df_filtered.groupby('item_name')['item_name'].transform('count')\n",
    "    df_filtered = df_filtered[df_filtered['no_features'] >= min_no_features]\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "def convert_column_values_string_to_rankInt(df) -> pd.DataFrame:\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == type(object):\n",
    "            le = preprocessing.LabelEncoder()\n",
    "            df[column] = le.fit_transform(df[column])\n",
    "    return df\n",
    "\n",
    "def flatten_column(df_column,method='median',round_base=2):\n",
    "    if method=='median':\n",
    "        return df_column.median()\n",
    "    elif method=='mean':\n",
    "        return round(df_column.mean(),round_base)\n",
    "    \n",
    "def interpolate_df(df,config):\n",
    "    feature_series = []\n",
    "    index = []\n",
    "    item_features = df.columns\n",
    "    for feature in item_features:\n",
    "        if feature in config['features']:\n",
    "            if config['features'][feature]=='median':\n",
    "                inter_series_f = flatten_column(df[feature],method='median')\n",
    "            elif config['features'][feature]=='mean':\n",
    "                inter_series_f = flatten_column(df[feature],method='mean')\n",
    "            else:\n",
    "                inter_series_f = flatten_column(df[feature],method=config['default_flatten'])\n",
    "        else:\n",
    "            inter_series_f = flatten_column(df[feature],method=config['default_flatten'])\n",
    "        \n",
    "        index.append(feature)\n",
    "        feature_series.append(inter_series_f)\n",
    "    return pd.Series(feature_series,index)\n",
    "\n",
    "def fill_and_plot(df,method='default',order=3):\n",
    "    if method in ['spline','polynomial']:\n",
    "        df_inter = df.interpolate(method=method,order=order)\n",
    "    else:\n",
    "        df_inter = df.interpolate(method=method)\n",
    "    plt.figure(figsize=(20,10))\n",
    "    df_inter['price_amount'].plot()\n",
    "    plt.legend([method])\n",
    "    return df_inter\n",
    "\n",
    "\n",
    "def fill_time_periods(df,method = 'pchip',order=3):\n",
    "\n",
    "    if method in ['spline','polynomial']:\n",
    "        df = df.interpolate(method=method,order=order)\n",
    "    else:\n",
    "        df = df.interpolate(method=method)\n",
    "    return df\n",
    "\n",
    "def feature_selection(df, method=\"decision_tree\",verbose=0,importance_threshold=0.15,max_no_of_features = 5):\n",
    "    important_features = []\n",
    "    print(\"Max number of features = {}\".format(max_no_of_features))\n",
    "    train = df.copy()\n",
    "    new_df = df.drop(['price_amount','time','date','socket_colors','time_hours','item_category'],axis=1,errors='ignore')\n",
    "    \n",
    "    if method=='custom':\n",
    "        df = new_df.copy()\n",
    "        df_missing = df!=0\n",
    "        df_missing_perc = df_missing.sum()/df.shape[0]\n",
    "        for value in df_missing_perc[df_missing_perc>=0.001].index.values:\n",
    "            important_features.append(value)\n",
    "        important_features.append('price_amount')\n",
    "        return important_features\n",
    "    elif method=='decision_tree':\n",
    "        model = RandomForestRegressor(random_state=20,max_depth=300)\n",
    "        new_df = pd.get_dummies(new_df)\n",
    "        model.fit(new_df,train.price_amount)\n",
    "        features = new_df.columns\n",
    "        importances = model.feature_importances_\n",
    "        indices = np.argsort(importances)[-max_no_of_features:]  # top 10 features\n",
    "        if verbose:\n",
    "            for i in indices:\n",
    "                print(\"Feature : {:40} --->importance [{}]\".format(features[i][:40],importances[i].round(3)))\n",
    "                      \n",
    "        for i in indices:\n",
    "            if importances[i] > importance_threshold:\n",
    "                important_features.append(features[i])\n",
    "        important_features.append('price_amount')\n",
    "        return important_features\n",
    "    \n",
    "    elif method=='rfe':\n",
    "        lreg = DecisionTreeRegressor()\n",
    "        rfe = RFE(lreg, max_no_of_features-1)\n",
    "        cols = new_df.columns\n",
    "        rfe = rfe.fit(new_df, train.price_amount)\n",
    "        sorted_ranking = sorted(zip(map(lambda x:round(x,5),rfe.ranking_),cols))\n",
    "        \n",
    "        for i in range(0,len(sorted_ranking)):\n",
    "            if verbose==1: print(\"Feature : {:40} has weight [{}]\".format(sorted_ranking[i][1][:35],sorted_ranking[i][0]))\n",
    "#             if (sorted_ranking[i][0] <= max_no_of_features*0.3 and (len(important_features)<(max_no_of_features))):    \n",
    "            if (len(important_features)<(max_no_of_features-1)):\n",
    "                important_features.append(sorted_ranking[i][1])\n",
    "                \n",
    "        important_features.append('price_amount')\n",
    "        return important_features\n",
    "    \n",
    "def dynamic_bins(df_bins, no_rows, min_percent=3, step=0.5, max_cum_step=50):\n",
    "    count = 0\n",
    "    left = 0\n",
    "    percent = 0\n",
    "    step_value = 0\n",
    "    bins = [0]\n",
    "    labels = []\n",
    "    for row in df_bins.iterrows():  \n",
    "\n",
    "        count = count + float(row[1]['count'])\n",
    "        right = float(row[0].right)  \n",
    "        calc_percent = (count / no_rows) * 100\n",
    "\n",
    "        #print(count, calc_percent)\n",
    "        if (calc_percent >= min_percent or step_value >= max_cum_step - 0.5):\n",
    "            bins.append(right)\n",
    "            labels.append('({},{}]'.format(left,right))\n",
    "#             print(\"{}\\t {}\\t {}\\t {}\".format(left,right, count,round(calc_percent,2) ))        \n",
    "            left = float(row[0].right)        \n",
    "            count = 0\n",
    "            step_value = 0\n",
    "            continue\n",
    "\n",
    "        step_value = step_value + step \n",
    "        \n",
    "    bins.append((right+max_cum_step))\n",
    "    labels.append('({},{}]'.format(left,right+max_cum_step))\n",
    "#     print(\"{}\\t {}\\t {}\\t {}\".format(left,right, count,round(calc_percent,2) ))       \n",
    "    return bins,labels\n",
    "\n",
    "\n",
    "def categorise_column(x,categories_dict):\n",
    "    no_of_good_features = 0\n",
    "    no_of_bad_features = 0\n",
    "    \n",
    "    for col,value in x.items():\n",
    "        if col in categories_dict and value>0:\n",
    "            if value>=categories_dict[col]['good_feature_value'] : \n",
    "                no_of_good_features = no_of_good_features +1\n",
    "            elif value<=categories_dict[col]['bad_feature_value'] : \n",
    "                no_of_bad_features = no_of_bad_features +1\n",
    "                \n",
    "    return no_of_good_features,no_of_bad_features\n",
    "\n",
    "def categorise_features_to_columns(x,categories_dict):\n",
    "    no_of_good_features = 0\n",
    "    no_of_bad_features = 0\n",
    "    \n",
    "    no_of_good_features,no_of_bad_features = categorise_column(x,categories_dict)\n",
    "    \n",
    "    return pd.Series([no_of_good_features,no_of_bad_features])\n",
    "\n",
    "\n",
    "def change_dict_path_value(dotted_path, org,value,delim='.'):\n",
    "    paths, current = dotted_path.split(sep=delim), org\n",
    "    for p in paths[:-1]:\n",
    "        if is_number(p) : current = current[int(p)]\n",
    "        else : current = current[p]\n",
    "    current[paths[-1]] = value\n",
    "    return org\n",
    "\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "def calculate_accuracy_and_acc_per_bucket(X_test,y_test,neural_network=True):\n",
    "    if neural_network : yhat = model.model.predict(X_test)\n",
    "    else : yhat = model.predict(X_test)\n",
    "    yhat_argmax = np.argmax(yhat,axis=1) \n",
    "    ytest_argmax = np.argmax(y_test,axis=1)\n",
    "    buckets = []\n",
    "    bucket_dict = {}\n",
    "    for x,y in zip(ytest_argmax,encoder.inverse_transform(ytest_argmax)):\n",
    "        if (x,y) not in buckets:\n",
    "            buckets.append((x,y))\n",
    "    buckets.sort(key=lambda tup: tup[0])\n",
    "\n",
    "    for bucket in buckets:\n",
    "        bucket_dict[bucket[1]] = [0,0]\n",
    "\n",
    "    count = 0\n",
    "    for yi,yihat in zip(encoder.inverse_transform(ytest_argmax),encoder.inverse_transform(yhat_argmax)):\n",
    "        if yi==yihat:\n",
    "            bucket_dict[yi][0] = bucket_dict[yi][0] + 1\n",
    "        else : \n",
    "            bucket_dict[yi][1] = bucket_dict[yi][1] + 1\n",
    "\n",
    "\n",
    "    list_of_keys = []\n",
    "    for key in list(bucket_dict.keys()):\n",
    "        list_of_keys.append(key)\n",
    "    accurracies_per_pricebucket = pd.DataFrame(bucket_dict,index=['correct_pred','wrong_pred'])\n",
    "    \n",
    "    accurracies_per_pricebucket = accurracies_per_pricebucket.transpose()\n",
    "    accurracies_per_pricebucket['bucket_accuracy'] = accurracies_per_pricebucket.apply((lambda x : x.correct_pred/(x.correct_pred+x.wrong_pred)*100),axis=1)\n",
    "    return accurracies_per_pricebucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T13:22:48.594135Z",
     "start_time": "2019-03-09T13:22:48.580102Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "\n",
    "    def __init__(self,configs,no_of_features,no_of_labels):\n",
    "        self.configs = configs\n",
    "        self.no_of_features = no_of_features\n",
    "        self.layer_count = 1\n",
    "        self.no_of_labels = no_of_labels\n",
    "#         self.model = Sequential()\n",
    "\n",
    "    def build_model(self):\n",
    "        configs = self.configs\n",
    "        no_of_features = self.no_of_features\n",
    "        optimizer = configs['model']['optimizer']\n",
    "        self.model = Sequential()\n",
    "        for layer in configs['model']['layers']:\n",
    "            neurons = layer['neurons'] if 'neurons' in layer else None\n",
    "            dropout_rate = layer['rate'] if 'rate' in layer else None\n",
    "            activation = layer['activation'] if 'activation' in layer else None\n",
    "            return_seq = layer['return_seq'] if 'return_seq' in layer else None\n",
    "            input_timesteps = layer['input_timesteps'] if 'input_timesteps' in layer else None\n",
    "            input_dim = layer['input_dim'] if 'input_dim' in layer else None\n",
    "            output_dim = layer['output_dim'] if 'output_dim' in layer else None\n",
    "            \n",
    "            if layer['type'] == 'dense':\n",
    "                if input_dim:\n",
    "                    self.model.add(Dense(neurons,input_dim=no_of_features, activation=activation))\n",
    "                    print(\"Layer {} -> Dense input layer\".format(self.layer_count))\n",
    "                    self.layer_count= self.layer_count+1\n",
    "                elif output_dim:\n",
    "                    self.model.add(Dense(self.no_of_labels, activation=activation))\n",
    "                    print(\"Layer {} -> Dense output layer\".format(self.layer_count))\n",
    "                else:\n",
    "                    self.model.add(Dense(neurons, activation=activation))\n",
    "                    if neurons==1 and activation: print(\"Layer {} -> Dense output layer. Activation -{}-\".format(self.layer_count,activation))\n",
    "                    else: print(\"Layer {} -> Dense hidden layer\".format(self.layer_count))\n",
    "                    self.layer_count= self.layer_count+1\n",
    "            if layer['type'] == 'lstm':\n",
    "                self.model.add(LSTM(neurons, input_shape=(input_timesteps, input_dim), return_sequences=return_seq))\n",
    "            if layer['type'] == 'dropout':\n",
    "                self.model.add(Dropout(dropout_rate))\n",
    "                print(\"Layer {} -> Dropout hidden layer\".format(self.layer_count))\n",
    "                self.layer_count= self.layer_count+1\n",
    "        if optimizer == 'SGD':\n",
    "            print(\"optimizer = SGD\")\n",
    "            optimizer = SGD(lr=0.01)\n",
    "        else:\n",
    "            print(\"Optimizer = {}\".format(optimizer))\n",
    "        print(\"Loss = {}\".format(configs['model']['loss']))\n",
    "        self.model.compile(loss=configs['model']['loss'], optimizer=optimizer,metrics=['mse','mae', 'accuracy'])\n",
    "        \n",
    "        print('[Model] Model Compiled')\n",
    "        print(self.model.summary())\n",
    "    \n",
    "#     def kfold_fit_model(self,X,Y,configs,no_of_features,regression=True,):\n",
    "#         epochs = configs['training']['epochs']\n",
    "#         batch_size = configs['training']['batch_size']\n",
    "#         seed = 7\n",
    "#         np.random.seed(seed)\n",
    "#         if regression:\n",
    "#             estimator = KerasRegressor(build_fn=self.build_model,epochs=epochs,batch_size=batch_size,verbose=1)\n",
    "#         else:\n",
    "#             estimator = KerasClassifier(build_fn=self.build_model,epochs=epochs,batch_size=batch_size,verbose=1)\n",
    "        \n",
    "# #         estimators = []\n",
    "# #         estimators.append(('standardize',MinMaxScaler(feature_range = (0, 1)) ))\n",
    "# #         estimators.append(('mlp',estimator))\n",
    "# #         pipeline = Pipeline(estimators)\n",
    "        \n",
    "#         kfold = KFold(n_splits=10, random_state=seed)\n",
    "#         results = cross_val_score(estimator, X, Y, cv=kfold)\n",
    "        \n",
    "#         print(\"Kfold results : %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n",
    "        \n",
    "    def fit_model(self,X_train,y_train,configs,no_of_features):\n",
    "        epochs = configs['training']['epochs']\n",
    "        batch_size = configs['training']['batch_size']\n",
    "        early_stop = EarlyStopping(monitor='accuracy',\n",
    "                              min_delta=0,\n",
    "                              patience=round(epochs*0.1),\n",
    "                              verbose=0, mode='auto')\n",
    "        history = self.model.fit(X_train,y_train,batch_size=batch_size,nb_epoch=epochs,callbacks=[early_stop])\n",
    "        plt.figure(figsize=(20,10))\n",
    "        plt.plot(history.history['mean_squared_error'])\n",
    "        plt.figure(figsize=(20,10))\n",
    "        plt.plot(history.history['mean_absolute_error'])\n",
    "        plt.show()\n",
    "        return history\n",
    "    \n",
    "    def predict_yhat(self,test_X):\n",
    "        return self.model.predict(test_X)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T13:22:49.510254Z",
     "start_time": "2019-03-09T13:22:49.506250Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# from keras.wrappers.scikit_learn import KerasRegressor,KerasClassifier\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "# class Model():\n",
    "\n",
    "#     def __init__(self,):\n",
    "#         self.model = Sequential()\n",
    "\n",
    "#     def build_model(self, configs,no_of_features):\n",
    "\n",
    "#         for layer in configs['model']['layers']:\n",
    "#             neurons = layer['neurons'] if 'neurons' in layer else None\n",
    "#             dropout_rate = layer['rate'] if 'rate' in layer else None\n",
    "#             activation = layer['activation'] if 'activation' in layer else None\n",
    "#             return_seq = layer['return_seq'] if 'return_seq' in layer else None\n",
    "#             input_timesteps = layer['input_timesteps'] if 'input_timesteps' in layer else None\n",
    "#             input_dim = layer['input_dim'] if 'input_dim' in layer else None\n",
    "\n",
    "#             if layer['type'] == 'dense':\n",
    "#                 print(\"Input dim = {}\".format(input_dim))\n",
    "#                 if input_dim:\n",
    "#                     self.model.add(Dense(neurons,input_dim=no_of_features, activation=activation))\n",
    "#                 else:\n",
    "#                     self.model.add(Dense(neurons, activation=activation))\n",
    "#             if layer['type'] == 'lstm':\n",
    "#                 self.model.add(LSTM(neurons, input_shape=(input_timesteps, input_dim), return_sequences=return_seq))\n",
    "#             if layer['type'] == 'dropout':\n",
    "#                 self.model.add(Dropout(dropout_rate))\n",
    "\n",
    "#         #self.model.compile(loss=configs['model']['loss'], optimizer=configs['model']['optimizer'],metrics=['mse', 'mae', 'mape'])\n",
    "        \n",
    "#         print('[Model] Model Compiled')\n",
    "        \n",
    "#     def kfold_fit_model(self,X,Y,configs,no_of_features,regression=True,):\n",
    "#         epochs = configs['training']['epochs']\n",
    "#         batch_size = configs['training']['batch_size']\n",
    "#         seed = 7\n",
    "#         np.random.seed(seed)\n",
    "#         if regression:\n",
    "#             estimator = KerasRegressor(build_fn=self.build_model(configs,no_of_features),epochs=epochs,batch_size=batch_size,verbose=1)\n",
    "#         else:\n",
    "#             estimator = KerasClassifier(build_fn=self.build_model(configs,no_of_features),epochs=epochs,batch_size=batch_size,verbose=1)\n",
    "        \n",
    "#         estimators = []\n",
    "#         estimators.append(('standardize',MinMaxScaler(feature_range = (0, 1)) ))\n",
    "#         estimators.append(('mlp',estimator))\n",
    "#         pipeline = Pipeline(estimators)\n",
    "        \n",
    "#         kfold = KFold(n_splits=10, random_state=seed)\n",
    "#         results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
    "        \n",
    "#         print(\"Kfold results : %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n",
    "        \n",
    "#     def fit_model(self,X_train,y_train,configs,regression=True):\n",
    "#         epochs = configs['training']['epochs']\n",
    "#         batch_size = configs['training']['batch_size']\n",
    "#         early_stop = EarlyStopping(monitor='mean_squared_error',\n",
    "#                               min_delta=0,\n",
    "#                               patience=round(epochs*0.2),\n",
    "#                               verbose=0, mode='auto')\n",
    "#         history = self.model.fit(X_train,y_train,batch_size=batch_size,nb_epoch=epochs,callbacks=[early_stop])\n",
    "#         plt.figure(figsize=(20,10))\n",
    "#         plt.plot(history.history['mean_squared_error'])\n",
    "#         plt.figure(figsize=(20,10))\n",
    "#         plt.plot(history.history['mean_absolute_error'])\n",
    "#         plt.figure(figsize=(20,10))\n",
    "#         plt.plot(history.history['mean_absolute_percentage_error'])\n",
    "#         plt.show()\n",
    "    \n",
    "#     def predict_yhat(self,test_X):\n",
    "#         return self.model.predict(test_X)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T13:22:49.685976Z",
     "start_time": "2019-03-09T13:22:49.664950Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    \"\"\"A class for loading and transforming data for the lstm model\"\"\"\n",
    "\n",
    "    def __init__(self, filename,config):\n",
    "        self.dataF = pd.read_csv(filename,delimiter='^').round(2)\n",
    "        self.prepared_data = self.prepare_data(self.dataF,config)\n",
    "        \n",
    "    def prepare_data(self,df,config):\n",
    "        \n",
    "        max_no_of_features = config['data']['no_of_features']\n",
    "        corr_threshold = config['data']['corr_threshold']\n",
    "        fs_method = config['data']['fs_method']\n",
    "        \n",
    "        #remove unwanted columns and rows\n",
    "        df = pre_process_df(df,verbose=0)\n",
    "                \n",
    "        #Remove outliers\n",
    "        outlier_conf = config['data']['outliers']\n",
    "        if outlier_conf['method']=='IQR':\n",
    "            df = remove_outliers_IQR(df,high_quantile=outlier_conf['high_quantile'])\n",
    "        elif outlier_conf['method']=='zscore':\n",
    "            df = remove_outliers_zscore(df,threshold=outlier_conf['threshold'])\n",
    "        elif outlier_conf['method']=='3_ex':\n",
    "            df = df[df['price_amount'] // df['ex_conv_rate']<= 3]\n",
    "        elif outlier_conf['method']=='brute':\n",
    "            df = df[df['price_amount'] // df['ex_conv_rate']<= 3]\n",
    "            df = df[df['price_amount'] >=0.5]\n",
    "        elif outlier_conf['method']=='cut_low':\n",
    "            low_pr_df = df.loc[(df['price_amount']<0.5) & (df['days_in_snapshot']>=3)]\n",
    "            df.drop(low_pr_df.index,inplace=True)\n",
    "        elif outlier_conf['method'] == 'quantile':\n",
    "            quantile_price = df['price_amount'].quantile(0.99)\n",
    "            df =df[df['price_amount']<= quantile_price]    \n",
    "        \n",
    "        #if a column has a high percentage of missing values make all values higher than 0(zero) equal to 1\n",
    "        if config['data']['high_percentage_missing_values']:\n",
    "            dataF = df.drop('price_amount',axis=1).copy()\n",
    "            cols = dataF.columns\n",
    "            count = 0\n",
    "            for col in cols:\n",
    "                column_zero_percentage = (dataF[col]!=0).sum()/(len(dataF[col]))*100\n",
    "                if column_zero_percentage<5:\n",
    "                    dataF[col] = dataF[col].apply(lambda x: 1 if x>0 else 0)\n",
    "                    count= count+1\n",
    "            dataF['price_amount'] = df['price_amount']\n",
    "            df = dataF.copy()\n",
    "        \n",
    "        if config['data']['feature_selection']:\n",
    "            \n",
    "            #Feature selection prep\n",
    "            important_features = feature_selection(df,method=fs_method, verbose=1,importance_threshold=corr_threshold,max_no_of_features=max_no_of_features)\n",
    "\n",
    "            #Actual feature selection\n",
    "            df = df[important_features]\n",
    "            print(\"Shape before {}\".format(df.shape))\n",
    "            \n",
    "            df['no_of_ex_features'] = (df.filter(regex='ex_.+')!=0).sum(axis=1)-1\n",
    "            df = df[df['no_of_ex_features']>0]\n",
    "            \n",
    "            print(\"Shape after {}\".format(df.shape))\n",
    "            print('Feature Selection method works')\n",
    "        \n",
    "        \n",
    "        feature_category_dict = {}\n",
    "        with open('bad_good_features_categegorization.json','r') as fjson:\n",
    "            feature_category_dict = json.load(fjson)\n",
    "        \n",
    "        df[['no_of_good_features','no_of_bad_features']] = df.apply(lambda x : categorise_features_to_columns(x,feature_category_dict),axis=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.no_features = df.shape[1]\n",
    "        \n",
    "        if config['data']['dynamic_price_bins']:\n",
    "            df['price_amount'] = self.convert_to_price_range(df,configs)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def rearrange_yhat_to_first_column(self,df,yhat_name='price_amount'):\n",
    "    \n",
    "        rearranged_columns = ['price_amount']\n",
    "        for c in df.columns:\n",
    "            if c=='price_amount' :continue\n",
    "            rearranged_columns.append(c)\n",
    "        return df[rearranged_columns]\n",
    "    \n",
    "    def split_X_Y(self):\n",
    "        rearranged_df_values = self.rearrange_yhat_to_first_column(self.prepared_data).values\n",
    "        X_data = rearranged_df_values[:,1:] \n",
    "        Y_data = rearranged_df_values[:,0]\n",
    "        return X_data,Y_data\n",
    "    \n",
    "    def convert_to_price_range(self,dataset,configs):\n",
    "        df = dataset.copy()\n",
    "        df = df[['price_amount']]\n",
    "        norm = 10\n",
    "        mn = 0\n",
    "        mx = int(round(df.max())) * norm\n",
    "        step = 0.5\n",
    "        step_adj = int(0.5 * norm)\n",
    "\n",
    "\n",
    "        bins = pd.cut(df['price_amount'], [x/10 for x in range(mn,mx, step_adj)])\n",
    "        df_bins = df.groupby(bins)['price_amount'].agg(['count'])\n",
    "        no_rows = df_bins['count'].sum()\n",
    "\n",
    "        df_bins['percent'] = round(df_bins  * 100 / no_rows, 2) \n",
    "\n",
    "        # --- FUNC params\n",
    "        min_percent = 3\n",
    "        no_rows = df_bins['count'].sum()\n",
    "        max_cum_step = 50\n",
    "        step = 0.5\n",
    "        \n",
    "        bins,labels = dynamic_bins(df_bins,no_rows,min_percent=configs['data']['min_percent'],max_cum_step=configs['data']['max_cum_step'])\n",
    "        df['price_range'] = pd.cut(df['price_amount'],bins=bins,labels=labels,include_lowest=True)\n",
    "        df.drop(['price_amount'],axis=1,inplace=True)\n",
    "        df.rename(columns={'price_range':'price_amount'},inplace=True)\n",
    "        self.no_of_labels = len(labels)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rare Analysis w/o Kfold and GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-16T15:42:33.544Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "\n",
    "configs = json.load(open('chest_class_config.json', 'r'))\n",
    "\n",
    "data = DataLoader(configs['data']['filename'],configs)\n",
    "\n",
    "#Trim data using \"days_in_snapshot\" feature\n",
    "data.prepared_data = data.prepared_data[(data.prepared_data['days_in_snapshot']<=2) &(data.prepared_data['days_in_snapshot']>=0)]\n",
    "\n",
    "no_of_classes = data.prepared_data['price_amount'].nunique()\n",
    "X,y = data.split_X_Y()\n",
    "\n",
    "no_of_features = X.shape[1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "svm = SVC(kernel='linear')\n",
    "\n",
    "# 2. Fit\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# 3. Predict \n",
    "y_pred = svm.predict(X_test)\n",
    "acc = accuracy_score(y_pred, y_test)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K- nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T16:35:42.512734Z",
     "start_time": "2019-02-16T16:28:12.539016Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "configs = json.load(open('chest_class_config.json', 'r'))\n",
    "\n",
    "data = DataLoader(configs['data']['filename'],configs)\n",
    "\n",
    "#Trim data using \"days_in_snapshot\" feature\n",
    "data.prepared_data = data.prepared_data[(data.prepared_data['days_in_snapshot']<=2) &(data.prepared_data['days_in_snapshot']>=0)]\n",
    "\n",
    "no_of_classes = data.prepared_data['price_amount'].nunique()\n",
    "X,y = data.split_X_Y()\n",
    "\n",
    "no_of_features = X.shape[1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "knn = KNeighborsClassifier(weights='distance',n_neighbors=1000,leaf_size=100)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-09T14:54:48.757Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(weights='distance',n_neighbors=no_of_classes,leaf_size=100)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering with KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T14:24:20.586573Z",
     "start_time": "2019-03-09T14:24:07.170590Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "configs = json.load(open('chest_class_config.json', 'r'))\n",
    "\n",
    "data = DataLoader(configs['data']['filename'],configs)\n",
    "\n",
    "#Trim data using \"days_in_snapshot\" feature\n",
    "data.prepared_data = data.prepared_data[(data.prepared_data['days_in_snapshot']<=2) &(data.prepared_data['days_in_snapshot']>=0)]\n",
    "\n",
    "no_of_classes = data.prepared_data['price_amount'].nunique()\n",
    "X,y = data.split_X_Y()\n",
    "\n",
    "no_of_features = X.shape[1]\n",
    "\n",
    "data.no_of_labels = full_data2.price_amount.nunique()\n",
    "\n",
    "no_of_classes = data.no_of_labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Instantiate\n",
    "kmc = KMeans(n_clusters=no_of_classes, max_iter=1000, n_init=20)\n",
    "\n",
    "# Fit\n",
    "kmc.fit(X_train, y_train)\n",
    "y_pred = kmc.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T14:27:29.448048Z",
     "start_time": "2019-03-09T14:24:40.988869Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "data.no_of_labels = full_data2.price_amount.nunique()\n",
    "\n",
    "no_of_classes = data.no_of_labels\n",
    "\n",
    "kmc = KMeans(n_clusters=no_of_classes, max_iter=1000, n_init=20)\n",
    "\n",
    "# Fit\n",
    "kmc.fit(X_train, y_train)\n",
    "y_pred = kmc.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T17:13:30.821761Z",
     "start_time": "2019-02-16T17:12:58.427549Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "configs = json.load(open('chest_class_config.json', 'r'))\n",
    "\n",
    "data = DataLoader(configs['data']['filename'],configs)\n",
    "\n",
    "#Trim data using \"days_in_snapshot\" feature\n",
    "data.prepared_data = data.prepared_data[(data.prepared_data['days_in_snapshot']<=2) &(data.prepared_data['days_in_snapshot']>=0)]\n",
    "\n",
    "no_of_classes = data.prepared_data['price_amount'].nunique()\n",
    "X,y = data.split_X_Y()\n",
    "\n",
    "no_of_features = X.shape[1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# 1. Instantiate\n",
    "# default criterion=gini\n",
    "# you can swap to criterion=entropy \n",
    "dtc = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "# 2. Fit\n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "# 3. Predict, there're 4 features in the iris dataset\n",
    "y_pred = dtc.predict(X_test)\n",
    "\n",
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBOOST with custom bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T09:10:37.662344Z",
     "start_time": "2019-03-01T08:47:10.090888Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from keras import backend as K\n",
    "K.clear_session()\n",
    "# load data\n",
    "configs = json.load(open('chest_class_config.json', 'r'))\n",
    "\n",
    "data = DataLoader(configs['data']['filename'],configs)\n",
    "full_data = data.prepared_data.copy()\n",
    "\n",
    "#Trim data using \"days_in_snapshot\" feature\n",
    "\n",
    "data.prepared_data = full_data[(full_data['days_in_snapshot']<=10) &(full_data['days_in_snapshot']>=0)]\n",
    "\n",
    "data.no_of_labels = full_data.price_amount.nunique()\n",
    "\n",
    "no_of_classes = data.no_of_labels\n",
    "X,y = data.split_X_Y()\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.1)\n",
    "sc = MinMaxScaler(feature_range=(0,1))\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# fit model no training data\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "# predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T13:28:51.796568Z",
     "start_time": "2019-03-09T13:23:35.966008Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "configs = json.load(open('chest_class_config.json', 'r'))\n",
    "\n",
    "data = DataLoader(configs['data']['filename'],configs)\n",
    "\n",
    "full_data = data.prepared_data.copy()\n",
    "\n",
    "#Trim data using \"days_in_snapshot\" feature\n",
    "\n",
    "data.prepared_data = full_data[(full_data['days_in_snapshot']<=10) &(full_data['days_in_snapshot']>=0)]\n",
    "\n",
    "data.no_of_labels = full_data.price_amount.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-03T22:11:47.056Z"
    },
    "code_folding": [
     14,
     28,
     41
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "all_accuracies_permutations = {}\n",
    "\n",
    "configs = json.load(open('chest_class_config.json', 'r'))\n",
    "\n",
    "grid = ParameterGrid({\"training.epochs\": [20,30,40,50],\n",
    "                      \"model.layers.5.activation\":['softmax'],\n",
    "                      \"data.outliers.method\":['nothing','IQR'],\n",
    "                     })\n",
    "\n",
    "for param_list in list(grid):\n",
    "        \n",
    "    for key,value in param_list.items():\n",
    "        configs = change_dict_path_value(key,configs,value)\n",
    "    \n",
    "    if K.backend() == 'tensorflow':\n",
    "        K.clear_session()\n",
    "\n",
    "    data.prepared_data = full_data2[(full_data2['days_in_snapshot']<=10) &(full_data2['days_in_snapshot']>=0)]\n",
    "\n",
    "    data.no_of_labels = full_data2.price_amount.nunique()\n",
    "\n",
    "    no_of_classes = data.no_of_labels\n",
    "    X,y = data.split_X_Y()\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(y)\n",
    "    encoded_Y = encoder.transform(y)\n",
    "    # convert integers to dummy variables (i.e. one hot encoded)\n",
    "    y = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "    no_of_features = X.shape[1]\n",
    "\n",
    "    \n",
    "    permutation_key = \"epochs{}_outliers{}_features{}\".format(configs['training']['epochs'],\n",
    "                                                           configs['data']['outliers']['method'],\n",
    "                                                           no_of_features)\n",
    "    \n",
    "    \n",
    "    #configs = json.load(open('chest_class_config.json', 'r'))\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.1)\n",
    "    sc = MinMaxScaler(feature_range=(0,1))\n",
    "    #sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "    print(\"X.shape[1] = {}\".format(X.shape[1]))\n",
    "    model = Model(configs,X.shape[1],no_of_classes)\n",
    "    model.build_model()\n",
    "    history = model.fit_model(X_train,y_train,configs,X.shape[1]) \n",
    "\n",
    "    \n",
    "    accuracies_per_bucket = calculate_accuracy_and_acc_per_bucket(X_test,y_test)\n",
    "    all_accuracies_permutations[permutation_key] = accuracies_per_bucket\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for key, value in all_accuracies_permutations.items():\n",
    "    key\n",
    "    value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-03T22:11:48.852Z"
    }
   },
   "outputs": [],
   "source": [
    "yhat = model.model.predict(X_test)\n",
    "yhat_argmax = np.argmax(yhat,axis=1) \n",
    "ytest_argmax = np.argmax(y_test,axis=1)\n",
    "metrics.accuracy_score(ytest_argmax,yhat_argmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial dependence calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = json.load(open('chest_config.json', 'r'))\n",
    "\n",
    "data = DataLoader(configs['data']['filename'],configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = data.prepared_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble.partial_dependence import plot_partial_dependence\n",
    "from sklearn.ensemble.partial_dependence import partial_dependence\n",
    "\n",
    "X,y = data.split_X_Y()\n",
    "no_of_features = X.shape[1]\n",
    "names = full_data.drop('price_amount',axis=1).columns\n",
    "print(\"Training GBRT...\")\n",
    "clf = GradientBoostingRegressor(n_estimators=100, max_depth=10,\n",
    "                                learning_rate=0.1, loss='ls',\n",
    "                                random_state=1)\n",
    "clf.fit(X, y)\n",
    "print(\" done.\")\n",
    "features = range(0,no_of_features,1)\n",
    "\n",
    "\n",
    "names = full_data.drop('price_amount',axis=1).columns[0]\n",
    "features=[0]\n",
    "for i in range(0,no_of_features):\n",
    "    features = [i]\n",
    "    names = full_data.drop('price_amount',axis=1).columns[i] +(' '*i)\n",
    "    fig, axs = plot_partial_dependence(clf, X, features,\n",
    "                                   feature_names=names,n_cols=10,\n",
    "                                   n_jobs=5, grid_resolution=10)\n",
    "    axs[0].set_xlabel(names)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## price_range bucketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-11T13:18:20.372541Z",
     "start_time": "2019-02-11T13:18:20.249884Z"
    }
   },
   "outputs": [],
   "source": [
    "df[df['price_amount']<=0.5][df['days_in_snapshot']>=2]['price_amount'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-11T13:47:54.217900Z",
     "start_time": "2019-02-11T13:47:53.949031Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out = pd.cut(df['price_amount'],bins=[0,0.5,10000])\n",
    "out_norm = out.value_counts(sort=False, normalize=False)\n",
    "# pyplot.figure(figsize=(40,20))\n",
    "out_norm.plot.bar(rot=0, color=\"b\", figsize=(30,15))\n",
    "# ax.set_xticklabels([c[1:-1].replace(\",\",\" to\") for c in out.cat.categories])\n",
    "pyplot.ylabel(\"pct\")\n",
    "# pyplot.figure(figsize=(40,20))\n",
    "out_norm\n",
    "# df['price_distr']\n",
    "# pyplot.bar(df['price_distr'],height=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-11T13:46:02.148905Z",
     "start_time": "2019-02-11T13:46:01.799450Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out = pd.cut(df['price_amount'],bins=[0,0.5,1,2,5,10,50,100,200,300,400,500,600,700,800,900,1000,10000])\n",
    "out_norm = out.value_counts(sort=False, normalize=True).mul(100)\n",
    "# pyplot.figure(figsize=(40,20))\n",
    "out_norm.plot.bar(rot=0, color=\"b\", figsize=(30,15))\n",
    "# ax.set_xticklabels([c[1:-1].replace(\",\",\" to\") for c in out.cat.categories])\n",
    "pyplot.ylabel(\"pct\")\n",
    "# pyplot.figure(figsize=(40,20))\n",
    "# df['price_distr']\n",
    "# pyplot.bar(df['price_distr'],height=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "234px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
